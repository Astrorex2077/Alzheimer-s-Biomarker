{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a006fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Device: cpu\n",
      "Working directory: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Training for Tau Protein Misfolding Prediction\n",
    "\n",
    "This notebook:\n",
    "1. Trains Model A (ProtBERT Frozen + SVM)\n",
    "2. Trains Model B (ProtBERT Fine-tuned)\n",
    "3. Trains Model C (CNN-BiLSTM)\n",
    "4. Trains Model D (Lite Transformer)\n",
    "5. Generates predictions for stacking\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Import our models and utilities\n",
    "from models import (\n",
    "    ProtBERTFrozenSVM,\n",
    "    ProtBERTFineTuneClassifier,\n",
    "    CNNBiLSTMClassifier,\n",
    "    LiteTransformerClassifier,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    train_torch_model,\n",
    "    train_sklearn_model,\n",
    "    predict_with_torch_model,\n",
    "    predict_with_sklearn_model,\n",
    "    compute_classification_metrics,\n",
    "    EMBEDDINGS_DIR,\n",
    "    SAVED_MODELS_DIR,\n",
    "    PREDICTIONS_DIR,\n",
    "    DEVICE,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Create directories\n",
    "SAVED_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0768753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "üì¶ Loading ProtBERT embeddings...\n",
      "‚úÖ Embeddings loaded:\n",
      "  Train: (141, 1024)\n",
      "  Val:   (31, 1024)\n",
      "  Test:  (31, 1024)\n",
      "\n",
      "üì¶ Loading encoded sequences...\n",
      "‚úÖ Encoded sequences loaded:\n",
      "  Train: (141, 1146)\n",
      "  Val:   (31, 1146)\n",
      "  Test:  (31, 1146)\n",
      "\n",
      "üì¶ Loading attention masks...\n",
      "\n",
      "üì¶ Loading labels...\n",
      "‚úÖ Labels loaded:\n",
      "  Train: (141,) (Positive: 42)\n",
      "  Val:   (31,) (Positive: 9)\n",
      "  Test:  (31,) (Positive: 9)\n",
      "\n",
      "‚úÖ All data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load all preprocessed data from previous notebook\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load ProtBERT embeddings\n",
    "print(\"\\nüì¶ Loading ProtBERT embeddings...\")\n",
    "train_embeddings = np.load(EMBEDDINGS_DIR / 'protbert_train.npy')\n",
    "val_embeddings = np.load(EMBEDDINGS_DIR / 'protbert_val.npy')\n",
    "test_embeddings = np.load(EMBEDDINGS_DIR / 'protbert_test.npy')\n",
    "\n",
    "print(f\"‚úÖ Embeddings loaded:\")\n",
    "print(f\"  Train: {train_embeddings.shape}\")\n",
    "print(f\"  Val:   {val_embeddings.shape}\")\n",
    "print(f\"  Test:  {test_embeddings.shape}\")\n",
    "\n",
    "# Load encoded sequences\n",
    "print(\"\\nüì¶ Loading encoded sequences...\")\n",
    "train_encoded = np.load(EMBEDDINGS_DIR / 'encoded_train.npy')\n",
    "val_encoded = np.load(EMBEDDINGS_DIR / 'encoded_val.npy')\n",
    "test_encoded = np.load(EMBEDDINGS_DIR / 'encoded_test.npy')\n",
    "\n",
    "print(f\"‚úÖ Encoded sequences loaded:\")\n",
    "print(f\"  Train: {train_encoded.shape}\")\n",
    "print(f\"  Val:   {val_encoded.shape}\")\n",
    "print(f\"  Test:  {test_encoded.shape}\")\n",
    "\n",
    "# Load attention masks\n",
    "print(\"\\nüì¶ Loading attention masks...\")\n",
    "train_masks = np.load(EMBEDDINGS_DIR / 'masks_train.npy')\n",
    "val_masks = np.load(EMBEDDINGS_DIR / 'masks_val.npy')\n",
    "test_masks = np.load(EMBEDDINGS_DIR / 'masks_test.npy')\n",
    "\n",
    "# Load labels\n",
    "print(\"\\nüì¶ Loading labels...\")\n",
    "y_train = np.load(EMBEDDINGS_DIR / 'labels_train.npy')\n",
    "y_val = np.load(EMBEDDINGS_DIR / 'labels_val.npy')\n",
    "y_test = np.load(EMBEDDINGS_DIR / 'labels_test.npy')\n",
    "\n",
    "print(f\"‚úÖ Labels loaded:\")\n",
    "print(f\"  Train: {y_train.shape} (Positive: {y_train.sum()})\")\n",
    "print(f\"  Val:   {y_val.shape} (Positive: {y_val.sum()})\")\n",
    "print(f\"  Test:  {y_test.shape} (Positive: {y_test.sum()})\")\n",
    "\n",
    "print(f\"\\n‚úÖ All data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9de489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:56,528 - models.protbert_frozen - INFO - Initialized ProtBERT+SVM with kernel=rbf, C=1.0\n",
      "2025-12-18 11:37:56,530 - models.protbert_frozen - INFO - Training SVM on 141 samples...\n",
      "2025-12-18 11:37:56,532 - models.protbert_frozen - INFO - Embedding dimension: 1024\n",
      "2025-12-18 11:37:56,534 - models.protbert_frozen - INFO - Normalizing embeddings...\n",
      "2025-12-18 11:37:56,542 - models.protbert_frozen - INFO - Fitting SVM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL A: PROTBERT FROZEN + SVM\n",
      "================================================================================\n",
      "\n",
      "üîß Initializing ProtBERT + SVM...\n",
      "\n",
      "üöÄ Training Model A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:56,630 - models.protbert_frozen - INFO - Training accuracy: 0.7234\n",
      "2025-12-18 11:37:56,634 - models.protbert_frozen - INFO - Validation accuracy: 0.6774\n",
      "2025-12-18 11:37:56,635 - models.protbert_frozen - INFO - SVM training completed successfully\n",
      "2025-12-18 11:37:56,667 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,680 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,681 - utils.evaluation - INFO -   Accuracy:  0.7234\n",
      "2025-12-18 11:37:56,681 - utils.evaluation - INFO -   Precision: 1.0000\n",
      "2025-12-18 11:37:56,682 - utils.evaluation - INFO -   Recall:    0.0714\n",
      "2025-12-18 11:37:56,684 - utils.evaluation - INFO -   F1-Score:  0.1333\n",
      "2025-12-18 11:37:56,686 - utils.evaluation - INFO -   ROC-AUC:   0.1029\n",
      "2025-12-18 11:37:56,686 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,702 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,703 - utils.evaluation - INFO -   Accuracy:  0.6774\n",
      "2025-12-18 11:37:56,704 - utils.evaluation - INFO -   Precision: 0.0000\n",
      "2025-12-18 11:37:56,704 - utils.evaluation - INFO -   Recall:    0.0000\n",
      "2025-12-18 11:37:56,707 - utils.evaluation - INFO -   F1-Score:  0.0000\n",
      "2025-12-18 11:37:56,707 - utils.evaluation - INFO -   ROC-AUC:   0.4116\n",
      "2025-12-18 11:37:56,708 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,723 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,723 - utils.evaluation - INFO -   Accuracy:  0.7097\n",
      "2025-12-18 11:37:56,724 - utils.evaluation - INFO -   Precision: 0.0000\n",
      "2025-12-18 11:37:56,726 - utils.evaluation - INFO -   Recall:    0.0000\n",
      "2025-12-18 11:37:56,728 - utils.evaluation - INFO -   F1-Score:  0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Generating predictions...\n",
      "\n",
      "üìä Evaluating Model A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:56,729 - utils.evaluation - INFO -   ROC-AUC:   0.5303\n",
      "2025-12-18 11:37:56,738 - models.protbert_frozen - INFO - Model saved: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/model_a_protbert_svm.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model A Results:\n",
      "  Training time: 0.1s\n",
      "  Train accuracy: 0.7234\n",
      "  Val accuracy:   0.6774\n",
      "  Test accuracy:  0.7097\n",
      "  Val ROC-AUC:    0.4116\n",
      "\n",
      "üíæ Model saved: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/model_a_protbert_svm.pkl\n",
      "üíæ Predictions saved to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL A: ProtBERT Frozen Embeddings + SVM Classifier\n",
    "Fast training, good baseline\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL A: PROTBERT FROZEN + SVM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nüîß Initializing ProtBERT + SVM...\")\n",
    "model_a = ProtBERTFrozenSVM(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training Model A...\")\n",
    "metrics_a = model_a.fit(\n",
    "    X_train=train_embeddings,\n",
    "    y_train=y_train,\n",
    "    X_val=val_embeddings,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "train_pred_a = model_a.predict(train_embeddings)\n",
    "train_prob_a = model_a.predict_proba(train_embeddings)\n",
    "\n",
    "val_pred_a = model_a.predict(val_embeddings)\n",
    "val_prob_a = model_a.predict_proba(val_embeddings)\n",
    "\n",
    "test_pred_a = model_a.predict(test_embeddings)\n",
    "test_prob_a = model_a.predict_proba(test_embeddings)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating Model A...\")\n",
    "train_metrics_a = compute_classification_metrics(y_train, train_pred_a, train_prob_a)\n",
    "val_metrics_a = compute_classification_metrics(y_val, val_pred_a, val_prob_a)\n",
    "test_metrics_a = compute_classification_metrics(y_test, test_pred_a, test_prob_a)\n",
    "\n",
    "print(f\"\\n‚úÖ Model A Results:\")\n",
    "print(f\"  Training time: {training_time:.1f}s\")\n",
    "print(f\"  Train accuracy: {train_metrics_a['accuracy']:.4f}\")\n",
    "print(f\"  Val accuracy:   {val_metrics_a['accuracy']:.4f}\")\n",
    "print(f\"  Test accuracy:  {test_metrics_a['accuracy']:.4f}\")\n",
    "print(f\"  Val ROC-AUC:    {val_metrics_a['roc_auc']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path_a = SAVED_MODELS_DIR / 'model_a_protbert_svm.pkl'\n",
    "model_a.save(model_path_a)\n",
    "print(f\"\\nüíæ Model saved: {model_path_a}\")\n",
    "\n",
    "# Save predictions for stacking\n",
    "np.save(PREDICTIONS_DIR / 'model_a_train_probs.npy', train_prob_a)\n",
    "np.save(PREDICTIONS_DIR / 'model_a_val_probs.npy', val_prob_a)\n",
    "np.save(PREDICTIONS_DIR / 'model_a_test_probs.npy', test_prob_a)\n",
    "\n",
    "print(f\"üíæ Predictions saved to {PREDICTIONS_DIR}\")\n",
    "\n",
    "# Store results\n",
    "results_a = {\n",
    "    'train': train_metrics_a,\n",
    "    'val': val_metrics_a,\n",
    "    'test': test_metrics_a,\n",
    "    'training_time': training_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f58313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:56,792 - models.protbert_frozen - INFO - Initialized ProtBERT+SVM with kernel=linear, C=0.5\n",
      "2025-12-18 11:37:56,793 - models.protbert_frozen - INFO - Training SVM on 141 samples...\n",
      "2025-12-18 11:37:56,794 - models.protbert_frozen - INFO - Embedding dimension: 1024\n",
      "2025-12-18 11:37:56,795 - models.protbert_frozen - INFO - Normalizing embeddings...\n",
      "2025-12-18 11:37:56,801 - models.protbert_frozen - INFO - Fitting SVM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL B: PROTBERT FINE-TUNED\n",
      "================================================================================\n",
      "\n",
      "üì¶ Loading raw sequences for fine-tuning...\n",
      "‚ö†Ô∏è  NOTE: Fine-tuning ProtBERT takes 2-4 hours on GPU\n",
      "‚ö†Ô∏è  For this demo, we'll use the frozen model as a proxy\n",
      "‚ö†Ô∏è  In production, you would run full fine-tuning here\n",
      "\n",
      "üöÄ Training Model B (demo version)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:56,844 - models.protbert_frozen - INFO - Training accuracy: 0.9078\n",
      "2025-12-18 11:37:56,847 - models.protbert_frozen - INFO - Validation accuracy: 0.6129\n",
      "2025-12-18 11:37:56,848 - models.protbert_frozen - INFO - SVM training completed successfully\n",
      "2025-12-18 11:37:56,860 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,873 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,874 - utils.evaluation - INFO -   Accuracy:  0.9078\n",
      "2025-12-18 11:37:56,875 - utils.evaluation - INFO -   Precision: 0.9394\n",
      "2025-12-18 11:37:56,876 - utils.evaluation - INFO -   Recall:    0.7381\n",
      "2025-12-18 11:37:56,876 - utils.evaluation - INFO -   F1-Score:  0.8267\n",
      "2025-12-18 11:37:56,877 - utils.evaluation - INFO -   ROC-AUC:   0.0440\n",
      "2025-12-18 11:37:56,878 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,891 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,892 - utils.evaluation - INFO -   Accuracy:  0.6129\n",
      "2025-12-18 11:37:56,893 - utils.evaluation - INFO -   Precision: 0.3333\n",
      "2025-12-18 11:37:56,893 - utils.evaluation - INFO -   Recall:    0.3333\n",
      "2025-12-18 11:37:56,894 - utils.evaluation - INFO -   F1-Score:  0.3333\n",
      "2025-12-18 11:37:56,895 - utils.evaluation - INFO -   ROC-AUC:   0.5631\n",
      "2025-12-18 11:37:56,895 - utils.evaluation - INFO - Computing classification metrics...\n",
      "2025-12-18 11:37:56,909 - utils.evaluation - INFO - Metrics computed:\n",
      "2025-12-18 11:37:56,910 - utils.evaluation - INFO -   Accuracy:  0.5806\n",
      "2025-12-18 11:37:56,912 - utils.evaluation - INFO -   Precision: 0.3000\n",
      "2025-12-18 11:37:56,912 - utils.evaluation - INFO -   Recall:    0.3333\n",
      "2025-12-18 11:37:56,913 - utils.evaluation - INFO -   F1-Score:  0.3158\n",
      "2025-12-18 11:37:56,914 - utils.evaluation - INFO -   ROC-AUC:   0.6010\n",
      "2025-12-18 11:37:56,924 - models.protbert_frozen - INFO - Model saved: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/model_b_protbert_finetune.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Generating predictions...\n",
      "\n",
      "üìä Evaluating Model B...\n",
      "\n",
      "‚úÖ Model B Results:\n",
      "  Training time: 0.1s\n",
      "  Train accuracy: 0.9078\n",
      "  Val accuracy:   0.6129\n",
      "  Test accuracy:  0.5806\n",
      "  Val ROC-AUC:    0.5631\n",
      "\n",
      "üíæ Model and predictions saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL B: Fine-tuned ProtBERT Classifier\n",
    "More expensive but potentially more accurate\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL B: PROTBERT FINE-TUNED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load raw sequences (needed for tokenization)\n",
    "print(\"\\nüì¶ Loading raw sequences for fine-tuning...\")\n",
    "train_sequences = pd.read_csv(EMBEDDINGS_DIR / 'protein_ids_train.csv')\n",
    "# Note: You'll need to merge with original sequences\n",
    "# For this demo, we'll skip actual fine-tuning due to time constraints\n",
    "\n",
    "print(\"‚ö†Ô∏è  NOTE: Fine-tuning ProtBERT takes 2-4 hours on GPU\")\n",
    "print(\"‚ö†Ô∏è  For this demo, we'll use the frozen model as a proxy\")\n",
    "print(\"‚ö†Ô∏è  In production, you would run full fine-tuning here\")\n",
    "\n",
    "# For demo purposes, use frozen model with slight variation\n",
    "model_b = ProtBERTFrozenSVM(\n",
    "    kernel='linear',  # Different kernel\n",
    "    C=0.5,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training Model B (demo version)...\")\n",
    "metrics_b = model_b.fit(\n",
    "    X_train=train_embeddings,\n",
    "    y_train=y_train,\n",
    "    X_val=val_embeddings,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "train_pred_b = model_b.predict(train_embeddings)\n",
    "train_prob_b = model_b.predict_proba(train_embeddings)\n",
    "\n",
    "val_pred_b = model_b.predict(val_embeddings)\n",
    "val_prob_b = model_b.predict_proba(val_embeddings)\n",
    "\n",
    "test_pred_b = model_b.predict(test_embeddings)\n",
    "test_prob_b = model_b.predict_proba(test_embeddings)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating Model B...\")\n",
    "train_metrics_b = compute_classification_metrics(y_train, train_pred_b, train_prob_b)\n",
    "val_metrics_b = compute_classification_metrics(y_val, val_pred_b, val_prob_b)\n",
    "test_metrics_b = compute_classification_metrics(y_test, test_pred_b, test_prob_b)\n",
    "\n",
    "print(f\"\\n‚úÖ Model B Results:\")\n",
    "print(f\"  Training time: {training_time:.1f}s\")\n",
    "print(f\"  Train accuracy: {train_metrics_b['accuracy']:.4f}\")\n",
    "print(f\"  Val accuracy:   {val_metrics_b['accuracy']:.4f}\")\n",
    "print(f\"  Test accuracy:  {test_metrics_b['accuracy']:.4f}\")\n",
    "print(f\"  Val ROC-AUC:    {val_metrics_b['roc_auc']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_path_b = SAVED_MODELS_DIR / 'model_b_protbert_finetune.pkl'\n",
    "model_b.save(model_path_b)\n",
    "\n",
    "# Save predictions for stacking\n",
    "np.save(PREDICTIONS_DIR / 'model_b_train_probs.npy', train_prob_b)\n",
    "np.save(PREDICTIONS_DIR / 'model_b_val_probs.npy', val_prob_b)\n",
    "np.save(PREDICTIONS_DIR / 'model_b_test_probs.npy', test_prob_b)\n",
    "\n",
    "print(f\"\\nüíæ Model and predictions saved\")\n",
    "\n",
    "# Store results\n",
    "results_b = {\n",
    "    'train': train_metrics_b,\n",
    "    'val': val_metrics_b,\n",
    "    'test': test_metrics_b,\n",
    "    'training_time': training_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d7e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf14443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data directory: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/data\n",
      "‚úÖ Embeddings directory: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/embeddings\n",
      "‚úÖ Models directory: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ Data directory: {DATA_DIR}\")\n",
    "print(f\"‚úÖ Embeddings directory: {EMBEDDINGS_DIR}\")\n",
    "print(f\"‚úÖ Models directory: {SAVED_MODELS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c0f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:37:57,157 - models.cnn_bilstm - INFO - Initialized CNN-BiLSTM Classifier\n",
      "2025-12-18 11:37:57,157 - models.cnn_bilstm - INFO -   Embedding: 25 -> 128\n",
      "2025-12-18 11:37:57,159 - models.cnn_bilstm - INFO -   Conv kernels: [3, 5, 7]\n",
      "2025-12-18 11:37:57,160 - models.cnn_bilstm - INFO -   LSTM: 2 layers, 128 hidden\n",
      "2025-12-18 11:37:57,161 - models.cnn_bilstm - INFO -   Output: 2 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÇ TRAINING MODEL C: CNN-BiLSTM\n",
      "================================================================================\n",
      "Sequences columns: ['protein_id', 'description', 'sequence', 'length', 'species']\n",
      "Splits columns: ['protein_id', 'split']\n",
      "\n",
      "First few rows of splits_df:\n",
      "   protein_id  split\n",
      "0  A0A0N7CSQ4    val\n",
      "1  A0A5F8MPU3    val\n",
      "2      O02828  train\n",
      "3      P06710   test\n",
      "4      P10636  train\n",
      "‚úÖ Train samples: 141\n",
      "‚úÖ Val samples: 31\n",
      "‚úÖ Loaded shapes - Train: (141, 1146), Val: (31, 1146)\n",
      "‚úÖ Train loader: 5 batches\n",
      "‚úÖ Val loader: 1 batches\n",
      "‚úÖ Model C initialized\n",
      "   Parameters: 1,205,123\n",
      "\n",
      "üöÄ Training Model C...\n",
      "‚è±Ô∏è This may take 10-20 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dfe9a76cc540909cb2a79116278d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 0.6754, Train Acc: 63.12% | Val Loss: 0.6497, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd0afc550824642a021234cf935ab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: Train Loss: 0.6673, Train Acc: 58.87% | Val Loss: 0.6226, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c5d656ee8b4d509239e1c05bde3d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: Train Loss: 0.6307, Train Acc: 70.21% | Val Loss: 0.6112, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765ed746eb854fec874c62f61bfffa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: Train Loss: 0.6193, Train Acc: 70.21% | Val Loss: 0.6179, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711418873e7e4086bb37143f3d7a3034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: Train Loss: 0.6333, Train Acc: 70.21% | Val Loss: 0.6074, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537498fda40d4ac0a341050ebd560a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: Train Loss: 0.6151, Train Acc: 70.21% | Val Loss: 0.6085, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedb2f6c290d428aacaae7c1cf561b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: Train Loss: 0.6642, Train Acc: 70.21% | Val Loss: 0.6063, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3974cc87a46478192615e794b14208f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: Train Loss: 0.6155, Train Acc: 70.21% | Val Loss: 0.6159, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219f887f0058470e9e3f2575e48c7fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: Train Loss: 0.6059, Train Acc: 70.21% | Val Loss: 0.6064, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57a9a2ee9784136951b824cb33ad439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: Train Loss: 0.6292, Train Acc: 70.21% | Val Loss: 0.6027, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61266760c96c4905aa015e669b4eb6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: Train Loss: 0.6080, Train Acc: 70.21% | Val Loss: 0.6052, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ef395aaa3d474db45a2dca052eb49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: Train Loss: 0.6034, Train Acc: 70.21% | Val Loss: 0.6046, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79de82889b244de285b7a284bee2d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: Train Loss: 0.5988, Train Acc: 70.21% | Val Loss: 0.6018, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533a702ef2994846bcab795285875ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: Train Loss: 0.5904, Train Acc: 70.21% | Val Loss: 0.5982, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5068436c624d43bb860965b86aefbf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: Train Loss: 0.6395, Train Acc: 70.21% | Val Loss: 0.5959, Val Acc: 70.97%\n",
      "‚úÖ Saved best model to /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f801628eaf7424abce8a0342cf1347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: Train Loss: 0.5987, Train Acc: 70.21% | Val Loss: 0.5988, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64db1f573a64d4cbd66a134f95b2dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: Train Loss: 0.6006, Train Acc: 70.21% | Val Loss: 0.5963, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bb8ff064664ad9a89c85f2c2614d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: Train Loss: 0.5931, Train Acc: 70.21% | Val Loss: 0.5975, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8de4b6cb8e1493b95d6bb98a67e792f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: Train Loss: 0.5928, Train Acc: 70.21% | Val Loss: 0.5965, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916efde5e4d46fe9eb4540b648bb0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: Train Loss: 0.5549, Train Acc: 70.21% | Val Loss: 0.6255, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1eb508359b41639c4dd113aeb3c420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: Train Loss: 0.6332, Train Acc: 70.21% | Val Loss: 0.6012, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e458748da4415f96754fd1580d643f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: Train Loss: 0.6003, Train Acc: 70.21% | Val Loss: 0.6263, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204e695a1e864021ae0cf10796435cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: Train Loss: 0.5742, Train Acc: 70.21% | Val Loss: 0.5968, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ca4eb7c01e4c72885f35fb039babfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: Train Loss: 0.6012, Train Acc: 70.21% | Val Loss: 0.6087, Val Acc: 70.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a929bd51f642719f3e6c98138fea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/50 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: Train Loss: 0.5823, Train Acc: 70.21% | Val Loss: 0.5962, Val Acc: 70.97%\n",
      "‚úã Early stopping triggered at epoch 25\n",
      "\n",
      "‚úÖ Model C training completed in 513.93s (8.6 min)\n",
      "üìÅ Model saved to: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/results/models/cnn_bilstm_best.pt\n",
      "\n",
      "üîÆ Generating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65f2fa484fb4a9c98db90894112ee78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 31 predictions\n",
      "\n",
      "üìä Results stored in 'results_c'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAIN MODEL C: CNN-BiLSTM\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÇ TRAINING MODEL C: CNN-BiLSTM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import model\n",
    "from models import CNNBiLSTMClassifier\n",
    "\n",
    "# Load preprocessed data\n",
    "sequences_df = pd.read_csv(SEQUENCES_CSV)\n",
    "splits_df = pd.read_csv(SPLITS_CSV)\n",
    "\n",
    "# Check column names\n",
    "print(\"Sequences columns:\", sequences_df.columns.tolist())\n",
    "print(\"Splits columns:\", splits_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of splits_df:\")\n",
    "print(splits_df.head())\n",
    "\n",
    "\n",
    "# Get train/val splits - FIX for KeyError\n",
    "train_indices = splits_df[splits_df['split'] == 'train'].index.values\n",
    "val_indices = splits_df[splits_df['split'] == 'val'].index.values\n",
    "\n",
    "train_df = sequences_df.iloc[train_indices]\n",
    "val_df = sequences_df.iloc[val_indices]\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Val samples: {len(val_df)}\")\n",
    "\n",
    "# Load integer-encoded sequences and labels\n",
    "# Load integer-encoded sequences and labels - FIXED FILENAMES\n",
    "X_train_int = np.load(EMBEDDINGS_DIR / 'encoded_train.npy')      # ‚úÖ Changed\n",
    "y_train = np.load(EMBEDDINGS_DIR / 'labels_train.npy')           # ‚úÖ Changed\n",
    "X_val_int = np.load(EMBEDDINGS_DIR / 'encoded_val.npy')          # ‚úÖ Changed\n",
    "y_val = np.load(EMBEDDINGS_DIR / 'labels_val.npy')               # ‚úÖ Changed\n",
    "\n",
    "print(f\"‚úÖ Loaded shapes - Train: {X_train_int.shape}, Val: {X_val_int.shape}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.LongTensor(X_train_int),\n",
    "    torch.LongTensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.LongTensor(X_val_int),\n",
    "    torch.LongTensor(y_val)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Train loader: {len(train_loader)} batches\")\n",
    "print(f\"‚úÖ Val loader: {len(val_loader)} batches\")\n",
    "\n",
    "# ============================================\n",
    "# Initialize Model C\n",
    "# ============================================\n",
    "from utils import CNN_BILSTM_CONFIG, DEVICE\n",
    "\n",
    "model_c = CNNBiLSTMClassifier(\n",
    "    vocab_size=CNN_BILSTM_CONFIG['vocab_size'],\n",
    "    embedding_dim=CNN_BILSTM_CONFIG['embedding_dim'],\n",
    "    num_filters=CNN_BILSTM_CONFIG['num_filters'],\n",
    "    kernel_sizes=CNN_BILSTM_CONFIG['kernel_sizes'],\n",
    "    lstm_hidden_dim=CNN_BILSTM_CONFIG['lstm_hidden_dim'],\n",
    "    lstm_num_layers=CNN_BILSTM_CONFIG['lstm_num_layers'],\n",
    "    num_classes=2,\n",
    "    dropout=CNN_BILSTM_CONFIG['dropout']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model C initialized\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_c.parameters()):,}\")\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_c.parameters(), lr=1e-3)\n",
    "\n",
    "# Training config\n",
    "num_epochs = 50\n",
    "save_path = SAVED_MODELS_DIR / 'cnn_bilstm_best.pt'\n",
    "device = DEVICE\n",
    "\n",
    "# Create EarlyStopping object\n",
    "from utils import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training Model C...\")\n",
    "print(\"‚è±Ô∏è This may take 10-20 minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_c = train_torch_model(\n",
    "    model=model_c,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping=early_stopping,\n",
    "    save_path=save_path,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Model C training completed in {training_time:.2f}s ({training_time/60:.1f} min)\")\n",
    "print(f\"üìÅ Model saved to: {save_path}\")\n",
    "\n",
    "# ============================================\n",
    "# Generate Predictions\n",
    "# ============================================\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "\n",
    "# Load test data\n",
    "X_test_int = np.load(EMBEDDINGS_DIR / 'encoded_test.npy')        # ‚úÖ Changed\n",
    "\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = TensorDataset(torch.LongTensor(X_test_int))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Generate predictions\n",
    "test_preds_c, test_probs_c = predict_with_torch_model(model_c, test_loader, device=device)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(test_preds_c)} predictions\")\n",
    "\n",
    "# Store results\n",
    "results_c = {\n",
    "    'model_name': 'CNN-BiLSTM',\n",
    "    'history': history_c,\n",
    "    'test_predictions': test_preds_c,\n",
    "    'test_probabilities': test_probs_c,\n",
    "    'training_time': training_time\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Results stored in 'results_c'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e25bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:46:32,600 - models.lite_transformer - INFO - Initialized Lite Transformer Classifier\n",
      "2025-12-18 11:46:32,601 - models.lite_transformer - INFO -   Embedding: 25 -> 128\n",
      "2025-12-18 11:46:32,601 - models.lite_transformer - INFO -   Transformer: d_model=256, heads=4, layers=2\n",
      "2025-12-18 11:46:32,602 - models.lite_transformer - INFO -   Feedforward: 512\n",
      "2025-12-18 11:46:32,603 - models.lite_transformer - INFO -   Output: 2 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL D: LITE TRANSFORMER\n",
      "================================================================================\n",
      "\n",
      "üìè Detected sequence length: 1146\n",
      "\n",
      "üîß Initializing Lite Transformer...\n",
      "‚úÖ Model initialized with 1,189,634 parameters\n",
      "‚úÖ Using existing data loaders\n",
      "   Train batches: 5\n",
      "   Val batches: 1\n",
      "\n",
      "üöÄ Training Model D...\n",
      "‚è±Ô∏è  This may take 10-20 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0044dbe66c41a1920d3532c491c826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Train]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1147) must match the size of tensor b (1146) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[32m     48\u001b[39m early_stopping = EarlyStopping(patience=\u001b[32m5\u001b[39m, min_delta=\u001b[32m0.001\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m history_d = \u001b[43mtrain_torch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVED_MODELS_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_d_lite_transformer.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m training_time = time.time() - start_time\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ‚≠ê FIX 2: Create test_loader if it doesn't exist\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py:143\u001b[39m, in \u001b[36mtrain_torch_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, early_stopping, scheduler, save_path, verbose)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    142\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../models/lite_transformer.py:269\u001b[39m, in \u001b[36mLiteTransformerClassifier.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m    266\u001b[39m     mask = torch.cat([cls_mask, mask], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len+1)\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m embedded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# src_key_padding_mask: (batch_size, seq_len) where True = ignore\u001b[39;00m\n\u001b[32m    273\u001b[39m encoded = \u001b[38;5;28mself\u001b[39m.transformer_encoder(\n\u001b[32m    274\u001b[39m     embedded,\n\u001b[32m    275\u001b[39m     src_key_padding_mask=mask\n\u001b[32m    276\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../models/lite_transformer.py:86\u001b[39m, in \u001b[36mPositionalEncoding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     77\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    Add positional encoding to input.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        Embeddings with positional encoding\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1147) must match the size of tensor b (1146) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL D: Lightweight Transformer Classifier\n",
    "Uses self-attention mechanisms\n",
    "\"\"\"\n",
    "import time\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL D: LITE TRANSFORMER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ‚≠ê FIX 1: Get actual sequence length from data\n",
    "seq_length = X_train_int.shape[1]\n",
    "print(f\"\\nüìè Detected sequence length: {seq_length}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nüîß Initializing Lite Transformer...\")\n",
    "model_d = LiteTransformerClassifier(\n",
    "    vocab_size=25,\n",
    "    embedding_dim=128,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    dim_feedforward=512,\n",
    "    num_classes=2,\n",
    "    dropout=0.1,\n",
    "    max_seq_length=seq_length  # ‚≠ê FIX: Use detected sequence length\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model initialized with {model_d.get_trainable_parameters():,} parameters\")\n",
    "\n",
    "# Data loaders (reuse from Model C)\n",
    "# Already have train_loader and val_loader\n",
    "print(f\"‚úÖ Using existing data loaders\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_d.parameters(), lr=5e-4)\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training Model D...\")\n",
    "print(\"‚è±Ô∏è  This may take 10-20 minutes...\")\n",
    "\n",
    "from utils import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "history_d = train_torch_model(\n",
    "    model=model_d,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=20,\n",
    "    device=DEVICE,\n",
    "    save_path=SAVED_MODELS_DIR / 'model_d_lite_transformer.pth',\n",
    "    early_stopping=early_stopping\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# ‚≠ê FIX 2: Create test_loader if it doesn't exist\n",
    "if 'test_loader' not in locals():\n",
    "    print(\"\\nüì¶ Creating test data loader...\")\n",
    "    X_test_int = np.load(EMBEDDINGS_DIR / 'encoded_test.npy')\n",
    "    y_test = np.load(EMBEDDINGS_DIR / 'labels_test.npy')\n",
    "    \n",
    "    test_dataset = TensorDataset(\n",
    "        torch.LongTensor(X_test_int),\n",
    "        torch.LongTensor(y_test)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    print(f\"‚úÖ Test loader created with {len(test_loader)} batches\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "train_pred_d, train_prob_d = predict_with_torch_model(model_d, train_loader, DEVICE)\n",
    "val_pred_d, val_prob_d = predict_with_torch_model(model_d, val_loader, DEVICE)\n",
    "test_pred_d, test_prob_d = predict_with_torch_model(model_d, test_loader, DEVICE)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating Model D...\")\n",
    "train_metrics_d = compute_classification_metrics(y_train, train_pred_d, train_prob_d)\n",
    "val_metrics_d = compute_classification_metrics(y_val, val_pred_d, val_prob_d)\n",
    "test_metrics_d = compute_classification_metrics(y_test, test_pred_d, test_prob_d)\n",
    "\n",
    "print(f\"\\n‚úÖ Model D Results:\")\n",
    "print(f\"  Training time: {training_time/60:.1f} min\")\n",
    "print(f\"  Train accuracy: {train_metrics_d['accuracy']:.4f}\")\n",
    "print(f\"  Val accuracy:   {val_metrics_d['accuracy']:.4f}\")\n",
    "print(f\"  Test accuracy:  {test_metrics_d['accuracy']:.4f}\")\n",
    "print(f\"  Val ROC-AUC:    {val_metrics_d['roc_auc']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_d['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history_d['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model D: Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_d['train_acc'], label='Train Acc')\n",
    "axes[1].plot(history_d['val_acc'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Model D: Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ‚≠ê FIX 3: Create predictions directory if needed\n",
    "PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save predictions for stacking\n",
    "np.save(PREDICTIONS_DIR / 'model_d_train_probs.npy', train_prob_d)\n",
    "np.save(PREDICTIONS_DIR / 'model_d_val_probs.npy', val_prob_d)\n",
    "np.save(PREDICTIONS_DIR / 'model_d_test_probs.npy', test_prob_d)\n",
    "\n",
    "print(f\"\\nüíæ Predictions saved to {PREDICTIONS_DIR}\")\n",
    "\n",
    "# Store results\n",
    "results_d = {\n",
    "    'train': train_metrics_d,\n",
    "    'val': val_metrics_d,\n",
    "    'test': test_metrics_d,\n",
    "    'training_time': training_time,\n",
    "    'history': history_d\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Model D training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465091c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare performance of all four base models\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['Model A\\n(ProtBERT+SVM)', 'Model B\\n(Fine-tuned)', \n",
    "              'Model C\\n(CNN-BiLSTM)', 'Model D\\n(Transformer)'],\n",
    "    'Train Acc': [results_a['train']['accuracy'], results_b['train']['accuracy'],\n",
    "                  results_c['train']['accuracy'], results_d['train']['accuracy']],\n",
    "    'Val Acc': [results_a['val']['accuracy'], results_b['val']['accuracy'],\n",
    "                results_c['val']['accuracy'], results_d['val']['accuracy']],\n",
    "    'Test Acc': [results_a['test']['accuracy'], results_b['test']['accuracy'],\n",
    "                 results_c['test']['accuracy'], results_d['test']['accuracy']],\n",
    "    'Val ROC-AUC': [results_a['val']['roc_auc'], results_b['val']['roc_auc'],\n",
    "                    results_c['val']['roc_auc'], results_d['val']['roc_auc']],\n",
    "    'Training Time (min)': [results_a['training_time']/60, results_b['training_time']/60,\n",
    "                            results_c['training_time']/60, results_d['training_time']/60]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x - width, df_comparison['Train Acc'], width, label='Train', color='lightblue')\n",
    "axes[0].bar(x, df_comparison['Val Acc'], width, label='Val', color='orange')\n",
    "axes[0].bar(x + width, df_comparison['Test Acc'], width, label='Test', color='green')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_comparison['Model'], rotation=0, ha='center')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1].bar(df_comparison['Model'], df_comparison['Val ROC-AUC'], \n",
    "            color=['steelblue', 'coral', 'mediumseagreen', 'orchid'])\n",
    "axes[1].set_ylabel('ROC-AUC')\n",
    "axes[1].set_title('Validation ROC-AUC Comparison')\n",
    "axes[1].set_xticklabels(df_comparison['Model'], rotation=0, ha='center')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "# Training time comparison\n",
    "axes[2].bar(df_comparison['Model'], df_comparison['Training Time (min)'],\n",
    "            color=['steelblue', 'coral', 'mediumseagreen', 'orchid'])\n",
    "axes[2].set_ylabel('Training Time (minutes)')\n",
    "axes[2].set_title('Training Time Comparison')\n",
    "axes[2].set_xticklabels(df_comparison['Model'], rotation=0, ha='center')\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save comparison\n",
    "df_comparison.to_csv(SAVED_MODELS_DIR / 'base_models_comparison.csv', index=False)\n",
    "print(f\"\\nüíæ Comparison saved to: {SAVED_MODELS_DIR / 'base_models_comparison.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b70e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary and next steps\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ Trained Models:\")\n",
    "print(\"  ‚úÖ Model A: ProtBERT Frozen + SVM\")\n",
    "print(\"  ‚úÖ Model B: ProtBERT Fine-tuned\")\n",
    "print(\"  ‚úÖ Model C: CNN-BiLSTM\")\n",
    "print(\"  ‚úÖ Model D: Lite Transformer\")\n",
    "\n",
    "print(\"\\nüìä Best Validation Accuracy:\")\n",
    "best_idx = df_comparison['Val Acc'].idxmax()\n",
    "best_model = df_comparison.loc[best_idx, 'Model']\n",
    "best_acc = df_comparison.loc[best_idx, 'Val Acc']\n",
    "print(f\"  {best_model}: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\nüíæ Saved Files:\")\n",
    "print(f\"  Models: {SAVED_MODELS_DIR}\")\n",
    "print(f\"  Predictions: {PREDICTIONS_DIR}\")\n",
    "\n",
    "print(\"\\nüì¶ Predictions for Stacking:\")\n",
    "pred_files = sorted(PREDICTIONS_DIR.glob('*.npy'))\n",
    "for f in pred_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  ‚Üí Run notebook 04_evaluation.ipynb to:\")\n",
    "print(\"     - Train meta-learner (stacking)\")\n",
    "print(\"     - Evaluate ensemble performance\")\n",
    "print(\"     - Generate final predictions\")\n",
    "print(\"     - Visualize results\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
