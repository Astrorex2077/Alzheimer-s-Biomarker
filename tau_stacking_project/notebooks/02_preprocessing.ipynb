{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ea689",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing for Tau Protein Misfolding Prediction\n",
    "\n",
    "This notebook:\n",
    "1. Loads processed sequences and labels\n",
    "2. Creates train/val/test splits\n",
    "3. Generates ProtBERT embeddings\n",
    "4. Creates integer-encoded sequences\n",
    "5. Saves all preprocessed data\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our utilities\n",
    "from utils import (\n",
    "    load_core_tables,\n",
    "    make_splits,\n",
    "    save_core_tables,\n",
    "    compute_protbert_embeddings,\n",
    "    encode_sequences_to_int,\n",
    "    create_attention_masks,\n",
    "    compute_sequence_features,\n",
    "    save_embeddings_and_arrays,\n",
    "    SEQUENCES_CSV,\n",
    "    LABELS_CSV,\n",
    "    SPLITS_CSV,\n",
    "    EMBEDDINGS_DIR,\n",
    "    AMINO_ACID_VOCAB,\n",
    "    DEVICE,\n",
    "    TRAIN_RATIO,\n",
    "    VAL_RATIO,\n",
    "    TEST_RATIO,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bb244",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load sequences and labels from previous notebook\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: LOADING PROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load sequences and labels\n",
    "df_sequences = pd.read_csv(SEQUENCES_CSV)\n",
    "df_labels = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "print(f\"‚úÖ Loaded sequences: {len(df_sequences)} samples\")\n",
    "print(f\"‚úÖ Loaded labels: {len(df_labels)} samples\")\n",
    "\n",
    "# Merge\n",
    "df_data = df_sequences.merge(df_labels, on='protein_id')\n",
    "\n",
    "print(f\"\\nüìä Merged dataset:\")\n",
    "print(f\"  Total samples: {len(df_data)}\")\n",
    "print(f\"  Features: {df_data.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "print(df_data.head())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüîç Data types:\")\n",
    "print(df_data.dtypes)\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = df_data['protein_id'].duplicated().sum()\n",
    "print(f\"\\nüîç Duplicate protein IDs: {n_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec4815",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create stratified train/validation/test splits\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: CREATING DATA SPLITS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Split ratios:\")\n",
    "print(f\"  Train: {TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Val:   {VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test:  {TEST_RATIO*100:.0f}%\")\n",
    "\n",
    "# Create splits with stratification on labels\n",
    "df_with_splits = make_splits(\n",
    "    df_data,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    val_ratio=VAL_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    stratify_column='label'\n",
    ")\n",
    "\n",
    "# Display split statistics\n",
    "print(f\"\\nüìä Split statistics:\")\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    split_data = df_with_splits[df_with_splits['split'] == split_name]\n",
    "    n_samples = len(split_data)\n",
    "    n_positive = (split_data['label'] == 1).sum()\n",
    "    n_negative = (split_data['label'] == 0).sum()\n",
    "    \n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"  Total: {n_samples}\")\n",
    "    print(f\"  Label 0 (Normal): {n_negative} ({n_negative/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Label 1 (Misfolding): {n_positive} ({n_positive/n_samples*100:.1f}%)\")\n",
    "\n",
    "# Visualize splits\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Split sizes\n",
    "split_counts = df_with_splits['split'].value_counts()\n",
    "axes[0].bar(split_counts.index, split_counts.values, color=['steelblue', 'orange', 'green'])\n",
    "axes[0].set_xlabel('Split')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('Dataset Split Sizes')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Label distribution per split\n",
    "split_label_counts = df_with_splits.groupby(['split', 'label']).size().unstack()\n",
    "split_label_counts.plot(kind='bar', stacked=True, ax=axes[1], color=['lightgreen', 'salmon'])\n",
    "axes[1].set_xlabel('Split')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_title('Label Distribution per Split')\n",
    "axes[1].legend(['Normal (0)', 'Misfolding (1)'])\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save splits\n",
    "save_core_tables(\n",
    "    df_sequences=df_with_splits[['protein_id', 'description', 'sequence', 'length', 'species']],\n",
    "    df_labels=df_with_splits[['protein_id', 'label', 'source']],\n",
    "    df_splits=df_with_splits[['protein_id', 'split']]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved splits to: {SPLITS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97019ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate ProtBERT embeddings for TRAINING set\n",
    "This may take 10-30 minutes depending on dataset size and device\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: GENERATING PROTBERT EMBEDDINGS (TRAIN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get training sequences\n",
    "train_data = df_with_splits[df_with_splits['split'] == 'train']\n",
    "train_sequences = train_data['sequence'].tolist()\n",
    "\n",
    "print(f\"üî¨ Generating embeddings for {len(train_sequences)} training sequences...\")\n",
    "print(f\"‚è±Ô∏è  This may take 10-30 minutes...\")\n",
    "print(f\"üíª Using device: {DEVICE}\")\n",
    "\n",
    "# Generate embeddings\n",
    "cache_path = EMBEDDINGS_DIR / \"protbert_train.npy\"\n",
    "\n",
    "train_embeddings = compute_protbert_embeddings(\n",
    "    sequences=train_sequences,\n",
    "    batch_size=8,  # Adjust based on your GPU memory\n",
    "    pooling='mean',\n",
    "    device=DEVICE,\n",
    "    cache_path=cache_path,\n",
    "    use_cache=True  # Will load from cache if exists\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training embeddings generated!\")\n",
    "print(f\"   Shape: {train_embeddings.shape}\")\n",
    "print(f\"   Embedding dimension: {train_embeddings.shape[1]}\")\n",
    "print(f\"   Memory: {train_embeddings.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "# Visualize embedding statistics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_embeddings.mean(axis=1), bins=50, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Mean Embedding Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Mean Embedding Values')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_embeddings.std(axis=1), bins=50, color='orange', edgecolor='black')\n",
    "plt.xlabel('Std Dev of Embedding')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Embedding Std Deviations')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a3a45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate ProtBERT embeddings for VALIDATION and TEST sets\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: GENERATING PROTBERT EMBEDDINGS (VAL & TEST)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validation set\n",
    "print(\"\\nüî¨ Generating VALIDATION embeddings...\")\n",
    "val_data = df_with_splits[df_with_splits['split'] == 'val']\n",
    "val_sequences = val_data['sequence'].tolist()\n",
    "cache_path_val = EMBEDDINGS_DIR / \"protbert_val.npy\"\n",
    "\n",
    "val_embeddings = compute_protbert_embeddings(\n",
    "    sequences=val_sequences,\n",
    "    batch_size=8,\n",
    "    pooling='mean',\n",
    "    device=DEVICE,\n",
    "    cache_path=cache_path_val,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Validation embeddings: {val_embeddings.shape}\")\n",
    "\n",
    "# Test set\n",
    "print(\"\\nüî¨ Generating TEST embeddings...\")\n",
    "test_data = df_with_splits[df_with_splits['split'] == 'test']\n",
    "test_sequences = test_data['sequence'].tolist()\n",
    "cache_path_test = EMBEDDINGS_DIR / \"protbert_test.npy\"\n",
    "\n",
    "test_embeddings = compute_protbert_embeddings(\n",
    "    sequences=test_sequences,\n",
    "    batch_size=8,\n",
    "    pooling='mean',\n",
    "    device=DEVICE,\n",
    "    cache_path=cache_path_test,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test embeddings: {test_embeddings.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Embedding Summary:\")\n",
    "print(f\"  Train: {train_embeddings.shape}\")\n",
    "print(f\"  Val:   {val_embeddings.shape}\")\n",
    "print(f\"  Test:  {test_embeddings.shape}\")\n",
    "print(f\"  Total memory: {(train_embeddings.nbytes + val_embeddings.nbytes + test_embeddings.nbytes) / 1024**2:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853025",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create integer-encoded sequences for CNN-BiLSTM and Transformer models\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: INTEGER ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Determine max sequence length\n",
    "all_lengths = df_with_splits['length'].values\n",
    "max_length = int(np.percentile(all_lengths, 95))  # Use 95th percentile to avoid extreme outliers\n",
    "print(f\"Max sequence length (95th percentile): {max_length}\")\n",
    "\n",
    "# Encode train sequences\n",
    "print(\"\\nüî¢ Encoding TRAIN sequences...\")\n",
    "train_encoded = encode_sequences_to_int(\n",
    "    sequences=train_sequences,\n",
    "    vocab=AMINO_ACID_VOCAB,\n",
    "    max_length=max_length,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "print(f\"‚úÖ Train encoded shape: {train_encoded.shape}\")\n",
    "\n",
    "# Encode validation sequences\n",
    "print(\"\\nüî¢ Encoding VALIDATION sequences...\")\n",
    "val_encoded = encode_sequences_to_int(\n",
    "    sequences=val_sequences,\n",
    "    vocab=AMINO_ACID_VOCAB,\n",
    "    max_length=max_length,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "print(f\"‚úÖ Val encoded shape: {val_encoded.shape}\")\n",
    "\n",
    "# Encode test sequences\n",
    "print(\"\\nüî¢ Encoding TEST sequences...\")\n",
    "test_encoded = encode_sequences_to_int(\n",
    "    sequences=test_sequences,\n",
    "    vocab=AMINO_ACID_VOCAB,\n",
    "    max_length=max_length,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "print(f\"‚úÖ Test encoded shape: {test_encoded.shape}\")\n",
    "\n",
    "# Create attention masks\n",
    "print(\"\\nüé≠ Creating attention masks...\")\n",
    "train_masks = create_attention_masks(train_encoded)\n",
    "val_masks = create_attention_masks(val_encoded)\n",
    "test_masks = create_attention_masks(test_encoded)\n",
    "\n",
    "print(f\"‚úÖ Masks created:\")\n",
    "print(f\"  Train: {train_masks.shape}\")\n",
    "print(f\"  Val:   {val_masks.shape}\")\n",
    "print(f\"  Test:  {test_masks.shape}\")\n",
    "\n",
    "# Visualize encoding\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Show first sequence encoding\n",
    "sample_idx = 0\n",
    "sample_encoded = train_encoded[sample_idx][:100]  # First 100 positions\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_encoded.reshape(1, -1), cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Token ID')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Sample')\n",
    "plt.title('Example: First 100 Positions of Encoded Sequence')\n",
    "\n",
    "# Token distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "unique, counts = np.unique(train_encoded.flatten(), return_counts=True)\n",
    "plt.bar(unique, counts, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Token ID')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Distribution in Training Set')\n",
    "plt.yscale('log')\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3e29c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute hand-crafted sequence features\n",
    "These can be used alongside deep learning features\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: COMPUTING HAND-CRAFTED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute features for all splits\n",
    "print(\"\\nüî¨ Computing features for all sequences...\")\n",
    "\n",
    "train_features = compute_sequence_features(train_sequences)\n",
    "val_features = compute_sequence_features(val_sequences)\n",
    "test_features = compute_sequence_features(test_sequences)\n",
    "\n",
    "print(f\"‚úÖ Feature shapes:\")\n",
    "print(f\"  Train: {train_features.shape}\")\n",
    "print(f\"  Val:   {val_features.shape}\")\n",
    "print(f\"  Test:  {test_features.shape}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(f\"\\nüìä Training set feature statistics:\")\n",
    "print(train_features.describe())\n",
    "\n",
    "# Visualize features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "features_to_plot = ['length', 'charged_pct', 'hydrophobic_pct', 'aromatic_pct']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot by label\n",
    "    train_data_with_label = train_data.copy()\n",
    "    train_data_with_label[feature] = train_features[feature].values\n",
    "    \n",
    "    for label in [0, 1]:\n",
    "        data = train_data_with_label[train_data_with_label['label'] == label][feature]\n",
    "        ax.hist(data, bins=30, alpha=0.6, \n",
    "                label=f\"Label {label} ({'Normal' if label == 0 else 'Misfolding'})\",\n",
    "                edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save features\n",
    "train_features.to_csv(EMBEDDINGS_DIR / 'features_train.csv', index=False)\n",
    "val_features.to_csv(EMBEDDINGS_DIR / 'features_val.csv', index=False)\n",
    "test_features.to_csv(EMBEDDINGS_DIR / 'features_test.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved features to {EMBEDDINGS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db16185",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save all preprocessed arrays and embeddings\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: SAVING PREPROCESSED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save encoded sequences\n",
    "print(\"\\nüíæ Saving encoded sequences...\")\n",
    "np.save(EMBEDDINGS_DIR / 'encoded_train.npy', train_encoded)\n",
    "np.save(EMBEDDINGS_DIR / 'encoded_val.npy', val_encoded)\n",
    "np.save(EMBEDDINGS_DIR / 'encoded_test.npy', test_encoded)\n",
    "\n",
    "# Save attention masks\n",
    "print(\"üíæ Saving attention masks...\")\n",
    "np.save(EMBEDDINGS_DIR / 'masks_train.npy', train_masks)\n",
    "np.save(EMBEDDINGS_DIR / 'masks_val.npy', val_masks)\n",
    "np.save(EMBEDDINGS_DIR / 'masks_test.npy', test_masks)\n",
    "\n",
    "# Save labels\n",
    "print(\"üíæ Saving labels...\")\n",
    "np.save(EMBEDDINGS_DIR / 'labels_train.npy', train_data['label'].values)\n",
    "np.save(EMBEDDINGS_DIR / 'labels_val.npy', val_data['label'].values)\n",
    "np.save(EMBEDDINGS_DIR / 'labels_test.npy', test_data['label'].values)\n",
    "\n",
    "# Save protein IDs for reference\n",
    "print(\"üíæ Saving protein IDs...\")\n",
    "pd.Series(train_data['protein_id'].values).to_csv(EMBEDDINGS_DIR / 'protein_ids_train.csv', index=False, header=['protein_id'])\n",
    "pd.Series(val_data['protein_id'].values).to_csv(EMBEDDINGS_DIR / 'protein_ids_val.csv', index=False, header=['protein_id'])\n",
    "pd.Series(test_data['protein_id'].values).to_csv(EMBEDDINGS_DIR / 'protein_ids_test.csv', index=False, header=['protein_id'])\n",
    "\n",
    "print(\"\\n‚úÖ All data saved successfully!\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\nüìÅ Saved files in {EMBEDDINGS_DIR}:\")\n",
    "saved_files = sorted(EMBEDDINGS_DIR.glob('*'))\n",
    "for f in saved_files:\n",
    "    size_mb = f.stat().st_size / 1024**2\n",
    "    print(f\"  - {f.name} ({size_mb:.1f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6cbf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary and next steps\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  Train samples: {len(train_data)}\")\n",
    "print(f\"  Val samples: {len(val_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")\n",
    "print(f\"  Total samples: {len(df_with_splits)}\")\n",
    "\n",
    "print(\"\\nüì¶ Generated Data:\")\n",
    "print(\"  1. ProtBERT embeddings (1024-dim)\")\n",
    "print(\"  2. Integer-encoded sequences\")\n",
    "print(\"  3. Attention masks\")\n",
    "print(\"  4. Hand-crafted features\")\n",
    "print(\"  5. Labels\")\n",
    "\n",
    "print(\"\\nüíæ Storage Summary:\")\n",
    "total_size = sum(f.stat().st_size for f in EMBEDDINGS_DIR.glob('*'))\n",
    "print(f\"  Total preprocessed data: {total_size / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  ‚Üí Run notebook 03_training.ipynb to:\")\n",
    "print(\"     - Train Model A (ProtBERT + SVM)\")\n",
    "print(\"     - Train Model B (ProtBERT Fine-tune)\")\n",
    "print(\"     - Train Model C (CNN-BiLSTM)\")\n",
    "print(\"     - Train Model D (Lite Transformer)\")\n",
    "print(\"     - Generate predictions for stacking\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
