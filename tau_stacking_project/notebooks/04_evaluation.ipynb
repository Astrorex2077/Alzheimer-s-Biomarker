{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec009620",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation and Stacking Ensemble for Tau Protein Misfolding Prediction\n",
    "\n",
    "This notebook:\n",
    "1. Loads all base model predictions\n",
    "2. Trains meta-learner (stacking)\n",
    "3. Evaluates ensemble performance\n",
    "4. Compares all models\n",
    "5. Generates visualizations and final results\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import our models and utilities\n",
    "from models import (\n",
    "    LogisticMetaLearner,\n",
    "    XGBoostMetaLearner,\n",
    "    MLPMetaLearner,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    compute_classification_metrics,\n",
    "    plot_roc_curve,\n",
    "    plot_multiple_roc_curves,\n",
    "    plot_confusion_matrix,\n",
    "    plot_accuracy_bar,\n",
    "    create_metrics_comparison_table,\n",
    "    export_predictions,\n",
    "    save_metrics_json,\n",
    "    EMBEDDINGS_DIR,\n",
    "    PREDICTIONS_DIR,\n",
    "    SAVED_MODELS_DIR,\n",
    "    METRICS_DIR,\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe367ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load predictions from all base models\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: LOADING BASE MODEL PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load labels\n",
    "print(\"\\nüì¶ Loading labels...\")\n",
    "y_train = np.load(EMBEDDINGS_DIR / 'labels_train.npy')\n",
    "y_val = np.load(EMBEDDINGS_DIR / 'labels_val.npy')\n",
    "y_test = np.load(EMBEDDINGS_DIR / 'labels_test.npy')\n",
    "\n",
    "print(f\"‚úÖ Labels loaded:\")\n",
    "print(f\"  Train: {len(y_train)}\")\n",
    "print(f\"  Val:   {len(y_val)}\")\n",
    "print(f\"  Test:  {len(y_test)}\")\n",
    "\n",
    "# Load Model A predictions\n",
    "print(\"\\nüì¶ Loading Model A (ProtBERT+SVM) predictions...\")\n",
    "train_prob_a = np.load(PREDICTIONS_DIR / 'model_a_train_probs.npy')\n",
    "val_prob_a = np.load(PREDICTIONS_DIR / 'model_a_val_probs.npy')\n",
    "test_prob_a = np.load(PREDICTIONS_DIR / 'model_a_test_probs.npy')\n",
    "print(f\"‚úÖ Model A: {train_prob_a.shape}\")\n",
    "\n",
    "# Load Model B predictions\n",
    "print(\"\\nüì¶ Loading Model B (Fine-tuned) predictions...\")\n",
    "train_prob_b = np.load(PREDICTIONS_DIR / 'model_b_train_probs.npy')\n",
    "val_prob_b = np.load(PREDICTIONS_DIR / 'model_b_val_probs.npy')\n",
    "test_prob_b = np.load(PREDICTIONS_DIR / 'model_b_test_probs.npy')\n",
    "print(f\"‚úÖ Model B: {train_prob_b.shape}\")\n",
    "\n",
    "# Load Model C predictions\n",
    "print(\"\\nüì¶ Loading Model C (CNN-BiLSTM) predictions...\")\n",
    "train_prob_c = np.load(PREDICTIONS_DIR / 'model_c_train_probs.npy')\n",
    "val_prob_c = np.load(PREDICTIONS_DIR / 'model_c_val_probs.npy')\n",
    "test_prob_c = np.load(PREDICTIONS_DIR / 'model_c_test_probs.npy')\n",
    "print(f\"‚úÖ Model C: {train_prob_c.shape}\")\n",
    "\n",
    "# Load Model D predictions\n",
    "print(\"\\nüì¶ Loading Model D (Transformer) predictions...\")\n",
    "train_prob_d = np.load(PREDICTIONS_DIR / 'model_d_train_probs.npy')\n",
    "val_prob_d = np.load(PREDICTIONS_DIR / 'model_d_val_probs.npy')\n",
    "test_prob_d = np.load(PREDICTIONS_DIR / 'model_d_test_probs.npy')\n",
    "print(f\"‚úÖ Model D: {train_prob_d.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All base model predictions loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c780",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Build meta-features for stacking ensemble\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: BUILDING META-FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stack all probabilities to create meta-features\n",
    "print(\"\\nüî® Building meta-features from base model predictions...\")\n",
    "\n",
    "# Training meta-features\n",
    "X_meta_train = np.hstack([\n",
    "    train_prob_a,\n",
    "    train_prob_b,\n",
    "    train_prob_c,\n",
    "    train_prob_d\n",
    "])\n",
    "\n",
    "# Validation meta-features\n",
    "X_meta_val = np.hstack([\n",
    "    val_prob_a,\n",
    "    val_prob_b,\n",
    "    val_prob_c,\n",
    "    val_prob_d\n",
    "])\n",
    "\n",
    "# Test meta-features\n",
    "X_meta_test = np.hstack([\n",
    "    test_prob_a,\n",
    "    test_prob_b,\n",
    "    test_prob_c,\n",
    "    test_prob_d\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Meta-features created:\")\n",
    "print(f\"  Train: {X_meta_train.shape}\")\n",
    "print(f\"  Val:   {X_meta_val.shape}\")\n",
    "print(f\"  Test:  {X_meta_test.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Model A: 2 probabilities\")\n",
    "print(f\"  Model B: 2 probabilities\")\n",
    "print(f\"  Model C: 2 probabilities\")\n",
    "print(f\"  Model D: 2 probabilities\")\n",
    "print(f\"  Total:   8 meta-features\")\n",
    "\n",
    "# Visualize meta-features\n",
    "print(\"\\nüìä Visualizing meta-feature distributions...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "feature_names = [\n",
    "    'Model A - Class 0', 'Model A - Class 1',\n",
    "    'Model B - Class 0', 'Model B - Class 1',\n",
    "    'Model C - Class 0', 'Model C - Class 1',\n",
    "    'Model D - Class 0', 'Model D - Class 1'\n",
    "]\n",
    "\n",
    "for idx, (ax, feature_name) in enumerate(zip(axes, feature_names)):\n",
    "    # Plot by true label\n",
    "    for label in [0, 1]:\n",
    "        mask = y_train == label\n",
    "        ax.hist(X_meta_train[mask, idx], bins=30, alpha=0.6,\n",
    "                label=f'Label {label}', edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(feature_name, fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5d49a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Logistic Regression meta-learner\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3A: TRAINING META-LEARNER (LOGISTIC REGRESSION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize logistic meta-learner\n",
    "print(\"\\nüîß Initializing Logistic Regression meta-learner...\")\n",
    "meta_logistic = LogisticMetaLearner(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training meta-learner...\")\n",
    "metrics_meta_log = meta_logistic.fit(\n",
    "    X_meta=X_meta_train,\n",
    "    y=y_train,\n",
    "    X_val=X_meta_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating ensemble predictions (Logistic)...\")\n",
    "ensemble_train_pred_log = meta_logistic.predict(X_meta_train)\n",
    "ensemble_train_prob_log = meta_logistic.predict_proba(X_meta_train)\n",
    "\n",
    "ensemble_val_pred_log = meta_logistic.predict(X_meta_val)\n",
    "ensemble_val_prob_log = meta_logistic.predict_proba(X_meta_val)\n",
    "\n",
    "ensemble_test_pred_log = meta_logistic.predict(X_meta_test)\n",
    "ensemble_test_prob_log = meta_logistic.predict_proba(X_meta_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating ensemble (Logistic)...\")\n",
    "train_metrics_ens_log = compute_classification_metrics(y_train, ensemble_train_pred_log, ensemble_train_prob_log)\n",
    "val_metrics_ens_log = compute_classification_metrics(y_val, ensemble_val_pred_log, ensemble_val_prob_log)\n",
    "test_metrics_ens_log = compute_classification_metrics(y_test, ensemble_test_pred_log, ensemble_test_prob_log)\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble (Logistic) Results:\")\n",
    "print(f\"  Train accuracy: {train_metrics_ens_log['accuracy']:.4f}\")\n",
    "print(f\"  Val accuracy:   {val_metrics_ens_log['accuracy']:.4f}\")\n",
    "print(f\"  Test accuracy:  {test_metrics_ens_log['accuracy']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:   {test_metrics_ens_log['roc_auc']:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nüîç Feature importance (meta-learner coefficients):\")\n",
    "feature_importance_log = meta_logistic.get_feature_importance()\n",
    "for idx, (name, importance) in enumerate(zip(feature_names, feature_importance_log)):\n",
    "    print(f\"  {name}: {importance:.4f}\")\n",
    "\n",
    "# Save meta-learner\n",
    "meta_logistic.save(SAVED_MODELS_DIR / 'meta_learner_logistic.pkl')\n",
    "print(f\"\\nüíæ Meta-learner saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c7df6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train XGBoost meta-learner\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3B: TRAINING META-LEARNER (XGBOOST)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize XGBoost meta-learner\n",
    "print(\"\\nüîß Initializing XGBoost meta-learner...\")\n",
    "meta_xgb = XGBoostMetaLearner(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nüöÄ Training XGBoost meta-learner...\")\n",
    "metrics_meta_xgb = meta_xgb.fit(\n",
    "    X_meta=X_meta_train,\n",
    "    y=y_train,\n",
    "    X_val=X_meta_val,\n",
    "    y_val=y_val,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating ensemble predictions (XGBoost)...\")\n",
    "ensemble_train_pred_xgb = meta_xgb.predict(X_meta_train)\n",
    "ensemble_train_prob_xgb = meta_xgb.predict_proba(X_meta_train)\n",
    "\n",
    "ensemble_val_pred_xgb = meta_xgb.predict(X_meta_val)\n",
    "ensemble_val_prob_xgb = meta_xgb.predict_proba(X_meta_val)\n",
    "\n",
    "ensemble_test_pred_xgb = meta_xgb.predict(X_meta_test)\n",
    "ensemble_test_prob_xgb = meta_xgb.predict_proba(X_meta_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüìä Evaluating ensemble (XGBoost)...\")\n",
    "train_metrics_ens_xgb = compute_classification_metrics(y_train, ensemble_train_pred_xgb, ensemble_train_prob_xgb)\n",
    "val_metrics_ens_xgb = compute_classification_metrics(y_val, ensemble_val_pred_xgb, ensemble_val_prob_xgb)\n",
    "test_metrics_ens_xgb = compute_classification_metrics(y_test, ensemble_test_pred_xgb, ensemble_test_prob_xgb)\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble (XGBoost) Results:\")\n",
    "print(f\"  Train accuracy: {train_metrics_ens_xgb['accuracy']:.4f}\")\n",
    "print(f\"  Val accuracy:   {val_metrics_ens_xgb['accuracy']:.4f}\")\n",
    "print(f\"  Test accuracy:  {test_metrics_ens_xgb['accuracy']:.4f}\")\n",
    "print(f\"  Test ROC-AUC:   {test_metrics_ens_xgb['roc_auc']:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nüîç Feature importance (XGBoost):\")\n",
    "feature_importance_xgb = meta_xgb.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance_xgb\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Meta-Learner Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save meta-learner\n",
    "meta_xgb.save(SAVED_MODELS_DIR / 'meta_learner_xgboost')\n",
    "print(f\"\\nüíæ XGBoost meta-learner saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218d64d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive comparison of all models\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate metrics for base models on test set\n",
    "print(\"\\nüìä Computing metrics for base models...\")\n",
    "\n",
    "# Model A\n",
    "pred_a = (test_prob_a[:, 1] > 0.5).astype(int)\n",
    "metrics_a = compute_classification_metrics(y_test, pred_a, test_prob_a)\n",
    "\n",
    "# Model B\n",
    "pred_b = (test_prob_b[:, 1] > 0.5).astype(int)\n",
    "metrics_b = compute_classification_metrics(y_test, pred_b, test_prob_b)\n",
    "\n",
    "# Model C\n",
    "pred_c = (test_prob_c[:, 1] > 0.5).astype(int)\n",
    "metrics_c = compute_classification_metrics(y_test, pred_c, test_prob_c)\n",
    "\n",
    "# Model D\n",
    "pred_d = (test_prob_d[:, 1] > 0.5).astype(int)\n",
    "metrics_d = compute_classification_metrics(y_test, pred_d, test_prob_d)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Model A\\n(ProtBERT+SVM)',\n",
    "        'Model B\\n(Fine-tuned)',\n",
    "        'Model C\\n(CNN-BiLSTM)',\n",
    "        'Model D\\n(Transformer)',\n",
    "        'Ensemble\\n(Logistic)',\n",
    "        'Ensemble\\n(XGBoost)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        metrics_a['accuracy'],\n",
    "        metrics_b['accuracy'],\n",
    "        metrics_c['accuracy'],\n",
    "        metrics_d['accuracy'],\n",
    "        test_metrics_ens_log['accuracy'],\n",
    "        test_metrics_ens_xgb['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        metrics_a['precision'],\n",
    "        metrics_b['precision'],\n",
    "        metrics_c['precision'],\n",
    "        metrics_d['precision'],\n",
    "        test_metrics_ens_log['precision'],\n",
    "        test_metrics_ens_xgb['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        metrics_a['recall'],\n",
    "        metrics_b['recall'],\n",
    "        metrics_c['recall'],\n",
    "        metrics_d['recall'],\n",
    "        test_metrics_ens_log['recall'],\n",
    "        test_metrics_ens_xgb['recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        metrics_a['f1_score'],\n",
    "        metrics_b['f1_score'],\n",
    "        metrics_c['f1_score'],\n",
    "        metrics_d['f1_score'],\n",
    "        test_metrics_ens_log['f1_score'],\n",
    "        test_metrics_ens_xgb['f1_score']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        metrics_a['roc_auc'],\n",
    "        metrics_b['roc_auc'],\n",
    "        metrics_c['roc_auc'],\n",
    "        metrics_d['roc_auc'],\n",
    "        test_metrics_ens_log['roc_auc'],\n",
    "        test_metrics_ens_xgb['roc_auc']\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìã Test Set Performance Comparison:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_idx = df_comparison['Accuracy'].idxmax()\n",
    "best_model = df_comparison.loc[best_idx, 'Model']\n",
    "best_acc = df_comparison.loc[best_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Save comparison\n",
    "df_comparison.to_csv(METRICS_DIR / 'all_models_comparison.csv', index=False)\n",
    "print(f\"\\nüíæ Comparison saved to: {METRICS_DIR / 'all_models_comparison.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0f17e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create comprehensive visualizations\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Accuracy comparison bar chart\n",
    "print(\"\\nüìä Creating accuracy comparison chart...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].barh(df_comparison['Model'], df_comparison['Accuracy'],\n",
    "                color=['steelblue']*4 + ['orange', 'red'])\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison', fontweight='bold')\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Precision & Recall\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, df_comparison['Precision'], width, label='Precision', color='skyblue')\n",
    "axes[0, 1].bar(x + width/2, df_comparison['Recall'], width, label='Recall', color='lightcoral')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_title('Precision vs Recall', fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(df_comparison['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# F1-Score\n",
    "axes[1, 0].barh(df_comparison['Model'], df_comparison['F1-Score'],\n",
    "                color=['steelblue']*4 + ['orange', 'red'])\n",
    "axes[1, 0].set_xlabel('F1-Score')\n",
    "axes[1, 0].set_title('F1-Score Comparison', fontweight='bold')\n",
    "axes[1, 0].set_xlim([0, 1])\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1, 1].barh(df_comparison['Model'], df_comparison['ROC-AUC'],\n",
    "                color=['steelblue']*4 + ['orange', 'red'])\n",
    "axes[1, 1].set_xlabel('ROC-AUC')\n",
    "axes[1, 1].set_title('ROC-AUC Comparison', fontweight='bold')\n",
    "axes[1, 1].set_xlim([0, 1])\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(METRICS_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved: {METRICS_DIR / 'model_comparison.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fede2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot ROC curves for all models\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä Creating ROC curves...\")\n",
    "\n",
    "# Prepare probabilities dict\n",
    "probs_dict = {\n",
    "    'Model A': test_prob_a,\n",
    "    'Model B': test_prob_b,\n",
    "    'Model C': test_prob_c,\n",
    "    'Model D': test_prob_d,\n",
    "    'Ensemble (Log)': ensemble_test_prob_log,\n",
    "    'Ensemble (XGB)': ensemble_test_prob_xgb\n",
    "}\n",
    "\n",
    "# Plot multiple ROC curves\n",
    "auc_scores = plot_multiple_roc_curves(\n",
    "    y_true=y_test,\n",
    "    y_probs_dict=probs_dict,\n",
    "    title='ROC Curves: All Models',\n",
    "    save_path=METRICS_DIR / 'roc_curves_all_models.png',\n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä ROC-AUC Scores:\")\n",
    "for model, score in auc_scores.items():\n",
    "    print(f\"  {model}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {METRICS_DIR / 'roc_curves_all_models.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ceb05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot confusion matrices for best models\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä Creating confusion matrices...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models_to_plot = [\n",
    "    ('Model A', pred_a),\n",
    "    ('Model B', pred_b),\n",
    "    ('Model C', pred_c),\n",
    "    ('Model D', pred_d),\n",
    "    ('Ensemble (Logistic)', ensemble_test_pred_log),\n",
    "    ('Ensemble (XGBoost)', ensemble_test_pred_xgb)\n",
    "]\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(models_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Misfolding'],\n",
    "                yticklabels=['Normal', 'Misfolding'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_title(f'{model_name}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(METRICS_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Saved: {METRICS_DIR / 'confusion_matrices.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f5cae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export final predictions with protein IDs\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: EXPORTING FINAL PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load protein IDs\n",
    "print(\"\\nüì¶ Loading protein IDs...\")\n",
    "protein_ids_test = pd.read_csv(EMBEDDINGS_DIR / 'protein_ids_test.csv')['protein_id'].values\n",
    "\n",
    "# Export ensemble predictions (XGBoost - best performer)\n",
    "print(\"\\nüíæ Exporting ensemble predictions...\")\n",
    "df_predictions = export_predictions(\n",
    "    protein_ids=protein_ids_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=ensemble_test_pred_xgb,\n",
    "    y_prob=ensemble_test_prob_xgb,\n",
    "    output_path=PREDICTIONS_DIR / 'final_ensemble_predictions.csv'\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Sample predictions:\")\n",
    "print(df_predictions.head(10))\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions exported: {PREDICTIONS_DIR / 'final_ensemble_predictions.csv'}\")\n",
    "\n",
    "# Save all metrics\n",
    "print(\"\\nüíæ Saving metrics to JSON...\")\n",
    "all_metrics = {\n",
    "    'model_a': metrics_a,\n",
    "    'model_b': metrics_b,\n",
    "    'model_c': metrics_c,\n",
    "    'model_d': metrics_d,\n",
    "    'ensemble_logistic': test_metrics_ens_log,\n",
    "    'ensemble_xgboost': test_metrics_ens_xgb\n",
    "}\n",
    "\n",
    "save_metrics_json(all_metrics, METRICS_DIR / 'final_metrics.json')\n",
    "print(f\"‚úÖ Metrics saved: {METRICS_DIR / 'final_metrics.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dee4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final summary and conclusions\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ Project Summary:\")\n",
    "print(f\"  Total samples: {len(y_train) + len(y_val) + len(y_test)}\")\n",
    "print(f\"  Train/Val/Test: {len(y_train)}/{len(y_val)}/{len(y_test)}\")\n",
    "\n",
    "print(\"\\nüèÜ Best Model Performance:\")\n",
    "print(f\"  Model: {best_model}\")\n",
    "print(f\"  Test Accuracy: {best_acc:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {df_comparison.loc[best_idx, 'ROC-AUC']:.4f}\")\n",
    "print(f\"  Test F1-Score: {df_comparison.loc[best_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Improvement from Base Models:\")\n",
    "base_models_max_acc = df_comparison.iloc[:4]['Accuracy'].max()\n",
    "ensemble_acc = df_comparison.iloc[5]['Accuracy']  # XGBoost\n",
    "improvement = (ensemble_acc - base_models_max_acc) / base_models_max_acc * 100\n",
    "\n",
    "print(f\"  Best base model accuracy: {base_models_max_acc:.4f}\")\n",
    "print(f\"  Ensemble accuracy: {ensemble_acc:.4f}\")\n",
    "print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\nüíæ Generated Files:\")\n",
    "print(f\"  Models: {SAVED_MODELS_DIR}\")\n",
    "print(f\"  Predictions: {PREDICTIONS_DIR}\")\n",
    "print(f\"  Metrics: {METRICS_DIR}\")\n",
    "print(f\"  Visualizations: {METRICS_DIR}\")\n",
    "\n",
    "print(\"\\nüìÅ Key Output Files:\")\n",
    "output_files = [\n",
    "    'all_models_comparison.csv',\n",
    "    'final_metrics.json',\n",
    "    'final_ensemble_predictions.csv',\n",
    "    'model_comparison.png',\n",
    "    'roc_curves_all_models.png',\n",
    "    'confusion_matrices.png'\n",
    "]\n",
    "\n",
    "for f in output_files:\n",
    "    print(f\"  ‚úÖ {f}\")\n",
    "\n",
    "print(\"\\nüéâ Tau Protein Misfolding Prediction Project Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
