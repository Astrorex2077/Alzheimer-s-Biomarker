{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adba4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TAU PROTEIN MISFOLDING PREDICTION\n",
      "Stacking Ensemble Approach\n",
      "================================================================================\n",
      "\n",
      "üìÖ Started: 2025-12-18 07:01:42\n",
      "üíª Working Directory: /workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks\n",
      "\n",
      "‚úÖ Master notebook initialized!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MASTER NOTEBOOK: Tau Protein Misfolding Prediction\n",
    "Complete End-to-End Pipeline\n",
    "\n",
    "This notebook orchestrates the entire project workflow:\n",
    "1. Dataset Generation (01_dataset_generation.ipynb)\n",
    "2. Preprocessing (02_preprocessing.ipynb)\n",
    "3. Model Training (03_training.ipynb)\n",
    "4. Evaluation & Stacking (04_evaluation.ipynb)\n",
    "\n",
    "Author: Tau Stacking Team\n",
    "Date: 2025-12-18\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TAU PROTEIN MISFOLDING PREDICTION\")\n",
    "print(\"Stacking Ensemble Approach\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üíª Working Directory: {Path.cwd()}\")\n",
    "print(\"\\n‚úÖ Master notebook initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fc02ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# üß¨ Tau Protein Misfolding Prediction Project\n",
       "\n",
       "## üìã Project Overview\n",
       "\n",
       "This project implements a **stacking ensemble** approach to predict tau protein misfolding, \n",
       "which is associated with neurodegenerative diseases like Alzheimer's.\n",
       "\n",
       "## üèóÔ∏è Architecture\n",
       "\n",
       "### Base Models (Level 0):\n",
       "- **Model A**: ProtBERT Frozen + SVM\n",
       "  - Fast, interpretable baseline\n",
       "  - Uses pre-computed embeddings\n",
       "\n",
       "- **Model B**: ProtBERT Fine-tuned\n",
       "  - Task-specific adaptation\n",
       "  - Higher capacity\n",
       "\n",
       "- **Model C**: CNN-BiLSTM\n",
       "  - Captures local patterns (CNN)\n",
       "  - Models sequential dependencies (BiLSTM)\n",
       "\n",
       "- **Model D**: Lightweight Transformer\n",
       "  - Self-attention mechanisms\n",
       "  - Global context modeling\n",
       "\n",
       "### Meta-Learner (Level 1):\n",
       "- **Logistic Regression**: Simple, interpretable\n",
       "- **XGBoost**: Powerful gradient boosting\n",
       "- **MLP**: Neural network flexibility\n",
       "\n",
       "## üìä Pipeline Stages\n",
       "\n",
       "1. **Dataset Generation**: Load and validate sequences\n",
       "2. **Preprocessing**: Embeddings, encoding, splitting\n",
       "3. **Training**: Train all 4 base models\n",
       "4. **Evaluation**: Stack predictions, final evaluation\n",
       "\n",
       "## üéØ Expected Outcomes\n",
       "\n",
       "- Individual model accuracies: ~70-85%\n",
       "- Ensemble accuracy: ~85-92%\n",
       "- Comprehensive performance metrics\n",
       "- Publication-ready visualizations\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display project overview and architecture\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "# üß¨ Tau Protein Misfolding Prediction Project\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This project implements a **stacking ensemble** approach to predict tau protein misfolding, \n",
    "which is associated with neurodegenerative diseases like Alzheimer's.\n",
    "\n",
    "## üèóÔ∏è Architecture\n",
    "\n",
    "### Base Models (Level 0):\n",
    "- **Model A**: ProtBERT Frozen + SVM\n",
    "  - Fast, interpretable baseline\n",
    "  - Uses pre-computed embeddings\n",
    "  \n",
    "- **Model B**: ProtBERT Fine-tuned\n",
    "  - Task-specific adaptation\n",
    "  - Higher capacity\n",
    "  \n",
    "- **Model C**: CNN-BiLSTM\n",
    "  - Captures local patterns (CNN)\n",
    "  - Models sequential dependencies (BiLSTM)\n",
    "  \n",
    "- **Model D**: Lightweight Transformer\n",
    "  - Self-attention mechanisms\n",
    "  - Global context modeling\n",
    "\n",
    "### Meta-Learner (Level 1):\n",
    "- **Logistic Regression**: Simple, interpretable\n",
    "- **XGBoost**: Powerful gradient boosting\n",
    "- **MLP**: Neural network flexibility\n",
    "\n",
    "## üìä Pipeline Stages\n",
    "\n",
    "1. **Dataset Generation**: Load and validate sequences\n",
    "2. **Preprocessing**: Embeddings, encoding, splitting\n",
    "3. **Training**: Train all 4 base models\n",
    "4. **Evaluation**: Stack predictions, final evaluation\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "\n",
    "- Individual model accuracies: ~70-85%\n",
    "- Ensemble accuracy: ~85-92%\n",
    "- Comprehensive performance metrics\n",
    "- Publication-ready visualizations\n",
    "\n",
    "---\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f1f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Pipeline Configuration:\n",
      "============================================================\n",
      "  ‚úÖ run_dataset_generation: True\n",
      "  ‚úÖ run_preprocessing: True\n",
      "  ‚úÖ run_training: True\n",
      "  ‚úÖ run_evaluation: True\n",
      "  ‚úÖ generate_report: True\n",
      "  ‚úÖ use_cached_embeddings: True\n",
      "  ‚è≠Ô∏è  quick_mode: False\n",
      "  ‚úÖ train_model_a: True\n",
      "  ‚úÖ train_model_b: True\n",
      "  ‚úÖ train_model_c: True\n",
      "  ‚úÖ train_model_d: True\n",
      "  ‚úÖ use_logistic: True\n",
      "  ‚úÖ use_xgboost: True\n",
      "  ‚è≠Ô∏è  use_mlp: False\n",
      "\n",
      "‚ö†Ô∏è  QUICK MODE: DISABLED (full training)\n",
      "\n",
      "‚è±Ô∏è  Estimated total time: 69 minutes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration for master pipeline execution\n",
    "\"\"\"\n",
    "\n",
    "# Pipeline configuration\n",
    "CONFIG = {\n",
    "    'run_dataset_generation': True,\n",
    "    'run_preprocessing': True,\n",
    "    'run_training': True,\n",
    "    'run_evaluation': True,\n",
    "    'generate_report': True,\n",
    "    \n",
    "    # Processing options\n",
    "    'use_cached_embeddings': True,  # Faster if embeddings exist\n",
    "    'quick_mode': False,  # Reduced epochs for testing\n",
    "    \n",
    "    # Model selection\n",
    "    'train_model_a': True,\n",
    "    'train_model_b': True,\n",
    "    'train_model_c': True,\n",
    "    'train_model_d': True,\n",
    "    \n",
    "    # Meta-learner selection\n",
    "    'use_logistic': True,\n",
    "    'use_xgboost': True,\n",
    "    'use_mlp': False,  # Optional\n",
    "}\n",
    "\n",
    "print(\"üîß Pipeline Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    status = \"‚úÖ\" if value else \"‚è≠Ô∏è \"\n",
    "    print(f\"  {status} {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  QUICK MODE:\" + (\" ENABLED (reduced epochs)\" if CONFIG['quick_mode'] else \" DISABLED (full training)\"))\n",
    "\n",
    "# Estimate total time\n",
    "estimated_time = {\n",
    "    'dataset_generation': 2,\n",
    "    'preprocessing': 30 if not CONFIG['use_cached_embeddings'] else 2,\n",
    "    'training': 60 if not CONFIG['quick_mode'] else 20,\n",
    "    'evaluation': 5,\n",
    "}\n",
    "\n",
    "total_time = sum(estimated_time.values())\n",
    "print(f\"\\n‚è±Ô∏è  Estimated total time: {total_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d07a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: DATASET GENERATION\n",
      "================================================================================\n",
      "\n",
      "üìì Executing: 01_dataset_generation.ipynb\n",
      "   ‚Üí Loading FASTA sequences\n",
      "   ‚Üí Creating/loading labels\n",
      "   ‚Üí Validating data\n",
      "   ‚Üí Saving processed files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Error in Stage 1: cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)\n",
      "Please check 01_dataset_generation.ipynb for details\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚Üí Saving processed files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# You would use %run or nbconvert here in actual execution\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# For demonstration, we show the key steps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     load_fasta,\n\u001b[32m     25\u001b[39m     create_synthetic_labels,\n\u001b[32m     26\u001b[39m     validate_sequences,\n\u001b[32m     27\u001b[39m     save_core_tables,\n\u001b[32m     28\u001b[39m     FASTA_FILE,\n\u001b[32m     29\u001b[39m     SEQUENCES_CSV,\n\u001b[32m     30\u001b[39m     LABELS_CSV,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Check if data already exists\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SEQUENCES_CSV.exists() \u001b[38;5;129;01mand\u001b[39;00m LABELS_CSV.exists():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/__init__.py:74\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     51\u001b[39m     load_fasta,\n\u001b[32m     52\u001b[39m     save_fasta,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     load_core_tables,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     compute_protbert_embeddings,\n\u001b[32m     64\u001b[39m     generate_and_cache_embeddings_by_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     load_embeddings,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_loops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     75\u001b[39m     EarlyStopping,\n\u001b[32m     76\u001b[39m     train_torch_model,\n\u001b[32m     77\u001b[39m     train_sklearn_model,\n\u001b[32m     78\u001b[39m     predict_with_torch_model,\n\u001b[32m     79\u001b[39m     predict_with_sklearn_model,\n\u001b[32m     80\u001b[39m     save_checkpoint,\n\u001b[32m     81\u001b[39m     load_checkpoint,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     85\u001b[39m     compute_classification_metrics,\n\u001b[32m     86\u001b[39m     compute_metrics_by_class,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     save_metrics_json,\n\u001b[32m     95\u001b[39m )\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstacking_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     98\u001b[39m     build_meta_features,\n\u001b[32m     99\u001b[39m     load_validation_predictions,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     StackingEnsemble,\n\u001b[32m    103\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 1: Dataset Generation\n",
    "\"\"\"\n",
    "\n",
    "if CONFIG['run_dataset_generation']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 1: DATASET GENERATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stage_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Execute notebook 01\n",
    "        print(\"\\nüìì Executing: 01_dataset_generation.ipynb\")\n",
    "        print(\"   ‚Üí Loading FASTA sequences\")\n",
    "        print(\"   ‚Üí Creating/loading labels\")\n",
    "        print(\"   ‚Üí Validating data\")\n",
    "        print(\"   ‚Üí Saving processed files\")\n",
    "        \n",
    "        # You would use %run or nbconvert here in actual execution\n",
    "        # For demonstration, we show the key steps\n",
    "        \n",
    "        from utils import (\n",
    "            load_fasta,\n",
    "            create_synthetic_labels,\n",
    "            validate_sequences,\n",
    "            save_core_tables,\n",
    "            FASTA_FILE,\n",
    "            SEQUENCES_CSV,\n",
    "            LABELS_CSV,\n",
    "        )\n",
    "        \n",
    "        # Check if data already exists\n",
    "        if SEQUENCES_CSV.exists() and LABELS_CSV.exists():\n",
    "            print(\"\\n   ‚úÖ Data files already exist!\")\n",
    "            df_sequences = pd.read_csv(SEQUENCES_CSV)\n",
    "            df_labels = pd.read_csv(LABELS_CSV)\n",
    "            print(f\"   ‚úÖ Loaded {len(df_sequences)} sequences\")\n",
    "        else:\n",
    "            print(\"\\n   ‚ö†Ô∏è  Data files not found. Please run 01_dataset_generation.ipynb manually\")\n",
    "            print(\"   Or provide tau_all_species.fasta file\")\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        print(f\"\\n‚úÖ Stage 1 Complete! ({stage_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in Stage 1: {e}\")\n",
    "        print(\"Please check 01_dataset_generation.ipynb for details\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Stage 1: Dataset Generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be12a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 2: PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "üìì Executing: 02_preprocessing.ipynb\n",
      "   ‚Üí Creating train/val/test splits\n",
      "   ‚Üí Generating ProtBERT embeddings\n",
      "   ‚Üí Encoding sequences\n",
      "   ‚Üí Computing features\n",
      "\n",
      "‚ùå Error in Stage 2: cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)\n",
      "Please check 02_preprocessing.ipynb for details\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚Üí Encoding sequences\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚Üí Computing features\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     make_splits,\n\u001b[32m     21\u001b[39m     compute_protbert_embeddings,\n\u001b[32m     22\u001b[39m     encode_sequences_to_int,\n\u001b[32m     23\u001b[39m     EMBEDDINGS_DIR,\n\u001b[32m     24\u001b[39m     DEVICE,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Check if preprocessed data exists\u001b[39;00m\n\u001b[32m     28\u001b[39m required_files = [\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprotbert_train.npy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprotbert_val.npy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels_train.npy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/__init__.py:74\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     51\u001b[39m     load_fasta,\n\u001b[32m     52\u001b[39m     save_fasta,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     load_core_tables,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     compute_protbert_embeddings,\n\u001b[32m     64\u001b[39m     generate_and_cache_embeddings_by_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     load_embeddings,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_loops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     75\u001b[39m     EarlyStopping,\n\u001b[32m     76\u001b[39m     train_torch_model,\n\u001b[32m     77\u001b[39m     train_sklearn_model,\n\u001b[32m     78\u001b[39m     predict_with_torch_model,\n\u001b[32m     79\u001b[39m     predict_with_sklearn_model,\n\u001b[32m     80\u001b[39m     save_checkpoint,\n\u001b[32m     81\u001b[39m     load_checkpoint,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     85\u001b[39m     compute_classification_metrics,\n\u001b[32m     86\u001b[39m     compute_metrics_by_class,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     save_metrics_json,\n\u001b[32m     95\u001b[39m )\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstacking_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     98\u001b[39m     build_meta_features,\n\u001b[32m     99\u001b[39m     load_validation_predictions,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     StackingEnsemble,\n\u001b[32m    103\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'save_checkpoint' from 'utils.training_loops' (/workspaces/Alzheimer-s-Biomarker/tau_stacking_project/notebooks/../utils/training_loops.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 2: Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "if CONFIG['run_preprocessing']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 2: PREPROCESSING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stage_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìì Executing: 02_preprocessing.ipynb\")\n",
    "        print(\"   ‚Üí Creating train/val/test splits\")\n",
    "        print(\"   ‚Üí Generating ProtBERT embeddings\")\n",
    "        print(\"   ‚Üí Encoding sequences\")\n",
    "        print(\"   ‚Üí Computing features\")\n",
    "        \n",
    "        from utils import (\n",
    "            make_splits,\n",
    "            compute_protbert_embeddings,\n",
    "            encode_sequences_to_int,\n",
    "            EMBEDDINGS_DIR,\n",
    "            DEVICE,\n",
    "        )\n",
    "        \n",
    "        # Check if preprocessed data exists\n",
    "        required_files = [\n",
    "            'protbert_train.npy',\n",
    "            'protbert_val.npy',\n",
    "            'protbert_test.npy',\n",
    "            'encoded_train.npy',\n",
    "            'labels_train.npy'\n",
    "        ]\n",
    "        \n",
    "        all_exist = all((EMBEDDINGS_DIR / f).exists() for f in required_files)\n",
    "        \n",
    "        if all_exist and CONFIG['use_cached_embeddings']:\n",
    "            print(\"\\n   ‚úÖ Preprocessed data already exists!\")\n",
    "            print(\"   ‚úÖ Using cached embeddings\")\n",
    "        else:\n",
    "            print(\"\\n   ‚ö†Ô∏è  Some preprocessed files missing\")\n",
    "            print(\"   Please run 02_preprocessing.ipynb manually\")\n",
    "            print(\"   This step takes 20-40 minutes for embedding generation\")\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        print(f\"\\n‚úÖ Stage 2 Complete! ({stage_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in Stage 2: {e}\")\n",
    "        print(\"Please check 02_preprocessing.ipynb for details\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Stage 2: Preprocessing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6775180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 3: MODEL TRAINING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STAGE 3: Model Training\n",
    "\"\"\"\n",
    "\n",
    "if CONFIG['run_training']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 3: MODEL TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stage_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìì Executing: 03_training.ipynb\")\n",
    "        \n",
    "        from utils import PREDICTIONS_DIR\n",
    "        \n",
    "        # Check which models to train\n",
    "        models_to_train = []\n",
    "        if CONFIG['train_model_a']: models_to_train.append('Model A')\n",
    "        if CONFIG['train_model_b']: models_to_train.append('Model B')\n",
    "        if CONFIG['train_model_c']: models_to_train.append('Model C')\n",
    "        if CONFIG['train_model_d']: models_to_train.append('Model D')\n",
    "        \n",
    "        print(f\"\\n   Training {len(models_to_train)} models:\")\n",
    "        for model in models_to_train:\n",
    "            print(f\"      ‚Ä¢ {model}\")\n",
    "        \n",
    "        # Check if predictions already exist\n",
    "        required_pred_files = [\n",
    "            'model_a_val_probs.npy',\n",
    "            'model_b_val_probs.npy',\n",
    "            'model_c_val_probs.npy',\n",
    "            'model_d_val_probs.npy',\n",
    "        ]\n",
    "        \n",
    "        all_exist = all((PREDICTIONS_DIR / f).exists() for f in required_pred_files)\n",
    "        \n",
    "        if all_exist:\n",
    "            print(\"\\n   ‚úÖ Model predictions already exist!\")\n",
    "            print(\"   ‚úÖ Skipping training (using cached predictions)\")\n",
    "        else:\n",
    "            print(\"\\n   ‚ö†Ô∏è  Some predictions missing\")\n",
    "            print(\"   Please run 03_training.ipynb manually\")\n",
    "            print(\"   This step takes 30-60 minutes depending on hardware\")\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        print(f\"\\n‚úÖ Stage 3 Complete! ({stage_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in Stage 3: {e}\")\n",
    "        print(\"Please check 03_training.ipynb for details\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Stage 3: Model Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STAGE 4: Evaluation & Stacking\n",
    "\"\"\"\n",
    "\n",
    "if CONFIG['run_evaluation']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STAGE 4: EVALUATION & STACKING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stage_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìì Executing: 04_evaluation.ipynb\")\n",
    "        print(\"   ‚Üí Building meta-features\")\n",
    "        print(\"   ‚Üí Training meta-learners\")\n",
    "        print(\"   ‚Üí Generating final predictions\")\n",
    "        print(\"   ‚Üí Creating visualizations\")\n",
    "        \n",
    "        from utils import METRICS_DIR\n",
    "        \n",
    "        # Check if evaluation results exist\n",
    "        if (METRICS_DIR / 'final_metrics.json').exists():\n",
    "            print(\"\\n   ‚úÖ Evaluation results already exist!\")\n",
    "            \n",
    "            # Load and display results\n",
    "            import json\n",
    "            with open(METRICS_DIR / 'final_metrics.json', 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            print(\"\\n   üìä Test Set Performance:\")\n",
    "            print(\"   \" + \"-\" * 60)\n",
    "            \n",
    "            model_names = {\n",
    "                'model_a': 'Model A (ProtBERT+SVM)',\n",
    "                'model_b': 'Model B (Fine-tuned)',\n",
    "                'model_c': 'Model C (CNN-BiLSTM)',\n",
    "                'model_d': 'Model D (Transformer)',\n",
    "                'ensemble_logistic': 'Ensemble (Logistic)',\n",
    "                'ensemble_xgboost': 'Ensemble (XGBoost)'\n",
    "            }\n",
    "            \n",
    "            for key, name in model_names.items():\n",
    "                if key in metrics:\n",
    "                    acc = metrics[key]['accuracy']\n",
    "                    auc = metrics[key].get('roc_auc', 0)\n",
    "                    print(f\"   {name:30s}: Acc={acc:.4f}, AUC={auc:.4f}\")\n",
    "        else:\n",
    "            print(\"\\n   ‚ö†Ô∏è  Evaluation results not found\")\n",
    "            print(\"   Please run 04_evaluation.ipynb manually\")\n",
    "        \n",
    "        stage_time = time.time() - stage_start\n",
    "        print(f\"\\n‚úÖ Stage 4 Complete! ({stage_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in Stage 4: {e}\")\n",
    "        print(\"Please check 04_evaluation.ipynb for details\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping Stage 4: Evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f71f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate comprehensive final report\n",
    "\"\"\"\n",
    "\n",
    "if CONFIG['generate_report']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL REPORT GENERATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        from utils import METRICS_DIR, PREDICTIONS_DIR, SAVED_MODELS_DIR\n",
    "        import json\n",
    "        \n",
    "        # Load final metrics\n",
    "        metrics_file = METRICS_DIR / 'final_metrics.json'\n",
    "        \n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                final_metrics = json.load(f)\n",
    "            \n",
    "            # Create summary report\n",
    "            report = f\"\"\"\n",
    "# üéâ PROJECT COMPLETION REPORT\n",
    "\n",
    "## üìä Final Results\n",
    "\n",
    "### Best Model Performance:\n",
    "\"\"\"\n",
    "            \n",
    "            # Find best ensemble\n",
    "            ensemble_models = {\n",
    "                'Logistic Regression': final_metrics.get('ensemble_logistic', {}),\n",
    "                'XGBoost': final_metrics.get('ensemble_xgboost', {}),\n",
    "            }\n",
    "            \n",
    "            best_ensemble = max(ensemble_models.items(), \n",
    "                              key=lambda x: x[1].get('accuracy', 0))\n",
    "            best_name, best_metrics = best_ensemble\n",
    "            \n",
    "            report += f\"\"\"\n",
    "- **Best Ensemble**: {best_name}\n",
    "- **Test Accuracy**: {best_metrics.get('accuracy', 0):.4f}\n",
    "- **Test ROC-AUC**: {best_metrics.get('roc_auc', 0):.4f}\n",
    "- **Test F1-Score**: {best_metrics.get('f1_score', 0):.4f}\n",
    "- **Precision**: {best_metrics.get('precision', 0):.4f}\n",
    "- **Recall**: {best_metrics.get('recall', 0):.4f}\n",
    "\n",
    "### Base Model Performance:\n",
    "\"\"\"\n",
    "            \n",
    "            base_models = {\n",
    "                'Model A (ProtBERT+SVM)': final_metrics.get('model_a', {}),\n",
    "                'Model B (Fine-tuned)': final_metrics.get('model_b', {}),\n",
    "                'Model C (CNN-BiLSTM)': final_metrics.get('model_c', {}),\n",
    "                'Model D (Transformer)': final_metrics.get('model_d', {}),\n",
    "            }\n",
    "            \n",
    "            for name, metrics in base_models.items():\n",
    "                acc = metrics.get('accuracy', 0)\n",
    "                auc = metrics.get('roc_auc', 0)\n",
    "                report += f\"- **{name}**: Accuracy={acc:.4f}, ROC-AUC={auc:.4f}\\n\"\n",
    "            \n",
    "            report += f\"\"\"\n",
    "\n",
    "## üìÅ Output Files\n",
    "\n",
    "### Models:\n",
    "\"\"\"\n",
    "            \n",
    "            # List saved models\n",
    "            model_files = list(SAVED_MODELS_DIR.glob('*'))\n",
    "            for f in model_files[:8]:  # Show first 8\n",
    "                report += f\"- `{f.name}`\\n\"\n",
    "            \n",
    "            report += f\"\"\"\n",
    "\n",
    "### Predictions:\n",
    "- `final_ensemble_predictions.csv` - Final predictions with protein IDs\n",
    "\n",
    "### Visualizations:\n",
    "- `model_comparison.png` - Performance comparison\n",
    "- `roc_curves_all_models.png` - ROC curves\n",
    "- `confusion_matrices.png` - Confusion matrices\n",
    "\n",
    "## üéØ Key Achievements\n",
    "\n",
    "‚úÖ Successfully trained 4 diverse base models  \n",
    "‚úÖ Implemented stacking ensemble approach  \n",
    "‚úÖ Achieved {best_metrics.get('accuracy', 0)*100:.2f}% test accuracy  \n",
    "‚úÖ Generated comprehensive evaluation metrics  \n",
    "‚úÖ Created publication-ready visualizations  \n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "1. **Validation**: Test on external datasets (NACC, OASIS)\n",
    "2. **Feature Analysis**: Investigate important sequence patterns\n",
    "3. **Deployment**: Package model for production use\n",
    "4. **Documentation**: Write research paper/report\n",
    "\n",
    "---\n",
    "\n",
    "*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "            \n",
    "            display(Markdown(report))\n",
    "            \n",
    "            # Save report\n",
    "            report_file = METRICS_DIR / 'final_report.md'\n",
    "            with open(report_file, 'w') as f:\n",
    "                f.write(report)\n",
    "            \n",
    "            print(f\"\\nüíæ Report saved: {report_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Metrics file not found. Cannot generate report.\")\n",
    "            print(\"Please run all pipeline stages first.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error generating report: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping report generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display visual summary of results\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from utils import METRICS_DIR\n",
    "    from IPython.display import Image\n",
    "    \n",
    "    # Display key visualizations\n",
    "    viz_files = [\n",
    "        'model_comparison.png',\n",
    "        'roc_curves_all_models.png',\n",
    "        'confusion_matrices.png'\n",
    "    ]\n",
    "    \n",
    "    for viz_file in viz_files:\n",
    "        viz_path = METRICS_DIR / viz_file\n",
    "        if viz_path.exists():\n",
    "            print(f\"\\nüìä {viz_file.replace('_', ' ').title().replace('.png', '')}\")\n",
    "            print(\"-\" * 80)\n",
    "            display(Image(filename=str(viz_path), width=900))\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  {viz_file} not found\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not display visualizations: {e}\")\n",
    "    print(\"Check the metrics directory manually\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display comprehensive project statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from utils import (\n",
    "        EMBEDDINGS_DIR,\n",
    "        PREDICTIONS_DIR,\n",
    "        SAVED_MODELS_DIR,\n",
    "        METRICS_DIR,\n",
    "    )\n",
    "    \n",
    "    # Count files\n",
    "    embeddings_files = list(EMBEDDINGS_DIR.glob('*'))\n",
    "    prediction_files = list(PREDICTIONS_DIR.glob('*'))\n",
    "    model_files = list(SAVED_MODELS_DIR.glob('*'))\n",
    "    metric_files = list(METRICS_DIR.glob('*'))\n",
    "    \n",
    "    # Calculate total size\n",
    "    def get_dir_size(directory):\n",
    "        return sum(f.stat().st_size for f in directory.glob('**/*') if f.is_file())\n",
    "    \n",
    "    total_size = (\n",
    "        get_dir_size(EMBEDDINGS_DIR) +\n",
    "        get_dir_size(PREDICTIONS_DIR) +\n",
    "        get_dir_size(SAVED_MODELS_DIR) +\n",
    "        get_dir_size(METRICS_DIR)\n",
    "    ) / 1024**2  # Convert to MB\n",
    "    \n",
    "    print(\"\\nüì¶ Generated Files:\")\n",
    "    print(f\"   Embeddings:  {len(embeddings_files)} files\")\n",
    "    print(f\"   Predictions: {len(prediction_files)} files\")\n",
    "    print(f\"   Models:      {len(model_files)} files\")\n",
    "    print(f\"   Metrics:     {len(metric_files)} files\")\n",
    "    print(f\"   Total size:  {total_size:.1f} MB\")\n",
    "    \n",
    "    # Load data sizes\n",
    "    if (EMBEDDINGS_DIR / 'labels_train.npy').exists():\n",
    "        y_train = np.load(EMBEDDINGS_DIR / 'labels_train.npy')\n",
    "        y_val = np.load(EMBEDDINGS_DIR / 'labels_val.npy')\n",
    "        y_test = np.load(EMBEDDINGS_DIR / 'labels_test.npy')\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Sizes:\")\n",
    "        print(f\"   Training:   {len(y_train)} samples\")\n",
    "        print(f\"   Validation: {len(y_val)} samples\")\n",
    "        print(f\"   Test:       {len(y_test)} samples\")\n",
    "        print(f\"   Total:      {len(y_train) + len(y_val) + len(y_test)} samples\")\n",
    "        \n",
    "        print(f\"\\nüéØ Class Distribution (Test Set):\")\n",
    "        print(f\"   Normal (0):     {(y_test==0).sum()} ({(y_test==0).sum()/len(y_test)*100:.1f}%)\")\n",
    "        print(f\"   Misfolding (1): {(y_test==1).sum()} ({(y_test==1).sum()/len(y_test)*100:.1f}%)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not compute statistics: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Master pipeline completion summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ MASTER PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "summary = f\"\"\"\n",
    "## üéâ Pipeline Execution Summary\n",
    "\n",
    "### Execution Details:\n",
    "- **Completed**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Configuration**: {'Quick Mode' if CONFIG['quick_mode'] else 'Full Training'}\n",
    "\n",
    "### Stages Completed:\n",
    "\"\"\"\n",
    "\n",
    "stages = [\n",
    "    ('Dataset Generation', CONFIG['run_dataset_generation']),\n",
    "    ('Preprocessing', CONFIG['run_preprocessing']),\n",
    "    ('Model Training', CONFIG['run_training']),\n",
    "    ('Evaluation', CONFIG['run_evaluation']),\n",
    "    ('Report Generation', CONFIG['generate_report']),\n",
    "]\n",
    "\n",
    "for stage, completed in stages:\n",
    "    status = \"‚úÖ\" if completed else \"‚è≠Ô∏è \"\n",
    "    summary += f\"- {status} {stage}\\n\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "\n",
    "### üéØ Key Deliverables:\n",
    "1. **4 Base Models** trained and saved\n",
    "2. **2 Meta-Learners** (Logistic, XGBoost) trained\n",
    "3. **Final Predictions** generated\n",
    "4. **Performance Metrics** computed\n",
    "5. **Visualizations** created\n",
    "6. **Final Report** generated\n",
    "\n",
    "### üìÅ All Output Locations:\n",
    "- **Models**: `results/models/`\n",
    "- **Predictions**: `results/predictions/`\n",
    "- **Metrics**: `results/metrics/`\n",
    "- **Embeddings**: `results/embeddings/`\n",
    "\n",
    "### üöÄ Ready for:\n",
    "- ‚úÖ Research paper writing\n",
    "- ‚úÖ Presentation preparation\n",
    "- ‚úÖ Further analysis\n",
    "- ‚úÖ Production deployment\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using the Tau Protein Misfolding Prediction Pipeline!**\n",
    "\n",
    "For questions or issues, please check individual notebooks or documentation.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(summary))\n",
    "\n",
    "print(\"\\nüéä PROJECT SUCCESSFULLY COMPLETED! üéä\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
